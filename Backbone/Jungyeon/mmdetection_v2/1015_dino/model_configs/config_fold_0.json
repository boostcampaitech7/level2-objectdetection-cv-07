{
    "dataset_type": "CocoDataset",
    "data_root": "data/coco/",
    "img_norm_cfg": {
        "mean": [
            123.675,
            116.28,
            103.53
        ],
        "std": [
            58.395,
            57.12,
            57.375
        ],
        "to_rgb": true
    },
    "train_pipeline": [
        {
            "type": "LoadImageFromFile"
        },
        {
            "type": "LoadAnnotations",
            "with_bbox": true
        },
        {
            "type": "RandomFlip",
            "prob": 0.5
        },
        {
            "type": "RandomChoice",
            "transforms": [
                [
                    {
                        "type": "RandomChoiceResize",
                        "scales": [
                            [
                                480,
                                1333
                            ],
                            [
                                512,
                                1333
                            ],
                            [
                                544,
                                1333
                            ],
                            [
                                576,
                                1333
                            ],
                            [
                                608,
                                1333
                            ],
                            [
                                640,
                                1333
                            ],
                            [
                                672,
                                1333
                            ],
                            [
                                704,
                                1333
                            ],
                            [
                                736,
                                1333
                            ],
                            [
                                768,
                                1333
                            ],
                            [
                                800,
                                1333
                            ]
                        ],
                        "keep_ratio": true
                    }
                ],
                [
                    {
                        "type": "RandomChoiceResize",
                        "scales": [
                            [
                                400,
                                4200
                            ],
                            [
                                500,
                                4200
                            ],
                            [
                                600,
                                4200
                            ]
                        ],
                        "keep_ratio": true
                    },
                    {
                        "type": "RandomCrop",
                        "crop_type": "absolute_range",
                        "crop_size": [
                            384,
                            600
                        ],
                        "allow_negative_crop": true
                    },
                    {
                        "type": "RandomChoiceResize",
                        "scales": [
                            [
                                480,
                                1333
                            ],
                            [
                                512,
                                1333
                            ],
                            [
                                544,
                                1333
                            ],
                            [
                                576,
                                1333
                            ],
                            [
                                608,
                                1333
                            ],
                            [
                                640,
                                1333
                            ],
                            [
                                672,
                                1333
                            ],
                            [
                                704,
                                1333
                            ],
                            [
                                736,
                                1333
                            ],
                            [
                                768,
                                1333
                            ],
                            [
                                800,
                                1333
                            ]
                        ],
                        "keep_ratio": true
                    }
                ]
            ]
        },
        {
            "type": "PackDetInputs"
        }
    ],
    "test_pipeline": [
        {
            "type": "LoadImageFromFile"
        },
        {
            "type": "MultiScaleFlipAug",
            "img_scale": [
                1333,
                800
            ],
            "flip": false,
            "transforms": [
                {
                    "type": "Resize",
                    "keep_ratio": true
                },
                {
                    "type": "RandomFlip"
                },
                {
                    "type": "Normalize",
                    "mean": [
                        123.675,
                        116.28,
                        103.53
                    ],
                    "std": [
                        58.395,
                        57.12,
                        57.375
                    ],
                    "to_rgb": true
                },
                {
                    "type": "Pad",
                    "size_divisor": 32
                },
                {
                    "type": "ImageToTensor",
                    "keys": [
                        "img"
                    ]
                },
                {
                    "type": "Collect",
                    "keys": [
                        "img"
                    ]
                }
            ]
        }
    ],
    "data": {
        "samples_per_gpu": 4,
        "workers_per_gpu": 2,
        "train": {
            "type": "CocoDataset",
            "ann_file": "Other/Dongjin/1010_split/result/train_0_1.json",
            "img_prefix": "/data/ephemeral/home/dataset",
            "pipeline": [
                {
                    "type": "LoadImageFromFile"
                },
                {
                    "type": "LoadAnnotations",
                    "with_bbox": true
                },
                {
                    "type": "Resize",
                    "img_scale": [
                        512,
                        512
                    ],
                    "keep_ratio": true
                },
                {
                    "type": "RandomFlip",
                    "flip_ratio": 0.5
                },
                {
                    "type": "Normalize",
                    "mean": [
                        123.675,
                        116.28,
                        103.53
                    ],
                    "std": [
                        58.395,
                        57.12,
                        57.375
                    ],
                    "to_rgb": true
                },
                {
                    "type": "Pad",
                    "size_divisor": 32
                },
                {
                    "type": "DefaultFormatBundle"
                },
                {
                    "type": "Collect",
                    "keys": [
                        "img",
                        "gt_bboxes",
                        "gt_labels"
                    ]
                }
            ],
            "classes": [
                "General trash",
                "Paper",
                "Paper pack",
                "Metal",
                "Glass",
                "Plastic",
                "Styrofoam",
                "Plastic bag",
                "Battery",
                "Clothing"
            ]
        },
        "val": {
            "type": "CocoDataset",
            "ann_file": "Other/Dongjin/1010_split/result/valid_0_1.json",
            "img_prefix": "/data/ephemeral/home/dataset",
            "pipeline": [
                {
                    "type": "LoadImageFromFile"
                },
                {
                    "type": "MultiScaleFlipAug",
                    "img_scale": [
                        512,
                        512
                    ],
                    "flip": false,
                    "transforms": [
                        {
                            "type": "Resize",
                            "keep_ratio": true
                        },
                        {
                            "type": "RandomFlip"
                        },
                        {
                            "type": "Normalize",
                            "mean": [
                                123.675,
                                116.28,
                                103.53
                            ],
                            "std": [
                                58.395,
                                57.12,
                                57.375
                            ],
                            "to_rgb": true
                        },
                        {
                            "type": "Pad",
                            "size_divisor": 32
                        },
                        {
                            "type": "ImageToTensor",
                            "keys": [
                                "img"
                            ]
                        },
                        {
                            "type": "Collect",
                            "keys": [
                                "img"
                            ]
                        }
                    ]
                }
            ],
            "classes": [
                "General trash",
                "Paper",
                "Paper pack",
                "Metal",
                "Glass",
                "Plastic",
                "Styrofoam",
                "Plastic bag",
                "Battery",
                "Clothing"
            ]
        },
        "test": {
            "type": "CocoDataset",
            "ann_file": "/data/ephemeral/home/dataset/test.json",
            "img_prefix": "/data/ephemeral/home/dataset",
            "pipeline": [
                {
                    "type": "LoadImageFromFile"
                },
                {
                    "type": "MultiScaleFlipAug",
                    "img_scale": [
                        512,
                        512
                    ],
                    "flip": false,
                    "transforms": [
                        {
                            "type": "Resize",
                            "keep_ratio": true
                        },
                        {
                            "type": "RandomFlip"
                        },
                        {
                            "type": "Normalize",
                            "mean": [
                                123.675,
                                116.28,
                                103.53
                            ],
                            "std": [
                                58.395,
                                57.12,
                                57.375
                            ],
                            "to_rgb": true
                        },
                        {
                            "type": "Pad",
                            "size_divisor": 32
                        },
                        {
                            "type": "ImageToTensor",
                            "keys": [
                                "img"
                            ]
                        },
                        {
                            "type": "Collect",
                            "keys": [
                                "img"
                            ]
                        }
                    ]
                }
            ],
            "classes": [
                "General trash",
                "Paper",
                "Paper pack",
                "Metal",
                "Glass",
                "Plastic",
                "Styrofoam",
                "Plastic bag",
                "Battery",
                "Clothing"
            ]
        }
    },
    "evaluation": {
        "interval": 1,
        "metric": "bbox"
    },
    "checkpoint_config": {
        "max_keep_ckpts": 3,
        "interval": 1
    },
    "log_config": {
        "interval": 50,
        "hooks": [
            {
                "type": "TextLoggerHook"
            }
        ]
    },
    "custom_hooks": [
        {
            "type": "NumClassCheckHook"
        }
    ],
    "dist_params": {
        "backend": "nccl"
    },
    "log_level": "INFO",
    "load_from": null,
    "resume_from": null,
    "workflow": [
        [
            "train",
            1
        ]
    ],
    "opencv_num_threads": 0,
    "mp_start_method": "fork",
    "auto_scale_lr": {
        "enable": false,
        "base_batch_size": 16
    },
    "model": {
        "type": "DINO",
        "num_queries": 900,
        "with_box_refine": true,
        "as_two_stage": true,
        "data_preprocessor": {
            "type": "DetDataPreprocessor",
            "mean": [
                123.675,
                116.28,
                103.53
            ],
            "std": [
                58.395,
                57.12,
                57.375
            ],
            "bgr_to_rgb": true,
            "pad_size_divisor": 1
        },
        "backbone": {
            "type": "SwinTransformer",
            "pretrain_img_size": 384,
            "embed_dims": 192,
            "depths": [
                2,
                2,
                18,
                2
            ],
            "num_heads": [
                6,
                12,
                24,
                48
            ],
            "window_size": 12,
            "mlp_ratio": 4,
            "qkv_bias": true,
            "qk_scale": null,
            "drop_rate": 0.0,
            "attn_drop_rate": 0.0,
            "drop_path_rate": 0.2,
            "patch_norm": true,
            "out_indices": [
                0,
                1,
                2,
                3
            ],
            "with_cp": true,
            "convert_weights": true,
            "init_cfg": {
                "type": "Pretrained",
                "checkpoint": "https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth"
            }
        },
        "neck": {
            "type": "ChannelMapper",
            "in_channels": [
                192,
                384,
                768,
                1536
            ],
            "kernel_size": 1,
            "out_channels": 256,
            "act_cfg": null,
            "norm_cfg": {
                "type": "GN",
                "num_groups": 32
            },
            "num_outs": 5
        },
        "encoder": {
            "num_layers": 6,
            "layer_cfg": {
                "self_attn_cfg": {
                    "embed_dims": 256,
                    "num_levels": 5,
                    "dropout": 0.0
                },
                "ffn_cfg": {
                    "embed_dims": 256,
                    "feedforward_channels": 2048,
                    "ffn_drop": 0.0
                }
            }
        },
        "decoder": {
            "num_layers": 6,
            "return_intermediate": true,
            "layer_cfg": {
                "self_attn_cfg": {
                    "embed_dims": 256,
                    "num_heads": 8,
                    "dropout": 0.0
                },
                "cross_attn_cfg": {
                    "embed_dims": 256,
                    "num_levels": 5,
                    "dropout": 0.0
                },
                "ffn_cfg": {
                    "embed_dims": 256,
                    "feedforward_channels": 2048,
                    "ffn_drop": 0.0
                }
            },
            "post_norm_cfg": null
        },
        "positional_encoding": {
            "num_feats": 128,
            "normalize": true,
            "offset": 0.0,
            "temperature": 20
        },
        "bbox_head": {
            "type": "DINOHead",
            "num_classes": 10,
            "sync_cls_avg_factor": true,
            "loss_cls": {
                "type": "FocalLoss",
                "use_sigmoid": true,
                "gamma": 2.0,
                "alpha": 0.25,
                "loss_weight": 1.0
            },
            "loss_bbox": {
                "type": "L1Loss",
                "loss_weight": 5.0
            },
            "loss_iou": {
                "type": "GIoULoss",
                "loss_weight": 2.0
            }
        },
        "dn_cfg": {
            "label_noise_scale": 0.5,
            "box_noise_scale": 1.0,
            "group_cfg": {
                "dynamic": true,
                "num_groups": null,
                "num_dn_queries": 100
            }
        },
        "train_cfg": {
            "assigner": {
                "type": "HungarianAssigner",
                "match_costs": [
                    {
                        "type": "FocalLossCost",
                        "weight": 2.0
                    },
                    {
                        "type": "BBoxL1Cost",
                        "weight": 5.0,
                        "box_format": "xywh"
                    },
                    {
                        "type": "IoUCost",
                        "iou_mode": "giou",
                        "weight": 2.0
                    }
                ]
            }
        },
        "test_cfg": {
            "max_per_img": 300
        },
        "num_feature_levels": 5
    },
    "train_dataloader": {
        "dataset": {
            "filter_cfg": {
                "filter_empty_gt": false
            },
            "pipeline": [
                {
                    "type": "LoadImageFromFile"
                },
                {
                    "type": "LoadAnnotations",
                    "with_bbox": true
                },
                {
                    "type": "RandomFlip",
                    "prob": 0.5
                },
                {
                    "type": "RandomChoice",
                    "transforms": [
                        [
                            {
                                "type": "RandomChoiceResize",
                                "scales": [
                                    [
                                        480,
                                        1333
                                    ],
                                    [
                                        512,
                                        1333
                                    ],
                                    [
                                        544,
                                        1333
                                    ],
                                    [
                                        576,
                                        1333
                                    ],
                                    [
                                        608,
                                        1333
                                    ],
                                    [
                                        640,
                                        1333
                                    ],
                                    [
                                        672,
                                        1333
                                    ],
                                    [
                                        704,
                                        1333
                                    ],
                                    [
                                        736,
                                        1333
                                    ],
                                    [
                                        768,
                                        1333
                                    ],
                                    [
                                        800,
                                        1333
                                    ]
                                ],
                                "keep_ratio": true
                            }
                        ],
                        [
                            {
                                "type": "RandomChoiceResize",
                                "scales": [
                                    [
                                        400,
                                        4200
                                    ],
                                    [
                                        500,
                                        4200
                                    ],
                                    [
                                        600,
                                        4200
                                    ]
                                ],
                                "keep_ratio": true
                            },
                            {
                                "type": "RandomCrop",
                                "crop_type": "absolute_range",
                                "crop_size": [
                                    384,
                                    600
                                ],
                                "allow_negative_crop": true
                            },
                            {
                                "type": "RandomChoiceResize",
                                "scales": [
                                    [
                                        480,
                                        1333
                                    ],
                                    [
                                        512,
                                        1333
                                    ],
                                    [
                                        544,
                                        1333
                                    ],
                                    [
                                        576,
                                        1333
                                    ],
                                    [
                                        608,
                                        1333
                                    ],
                                    [
                                        640,
                                        1333
                                    ],
                                    [
                                        672,
                                        1333
                                    ],
                                    [
                                        704,
                                        1333
                                    ],
                                    [
                                        736,
                                        1333
                                    ],
                                    [
                                        768,
                                        1333
                                    ],
                                    [
                                        800,
                                        1333
                                    ]
                                ],
                                "keep_ratio": true
                            }
                        ]
                    ]
                },
                {
                    "type": "PackDetInputs"
                }
            ]
        }
    },
    "optim_wrapper": {
        "type": "OptimWrapper",
        "optimizer": {
            "type": "AdamW",
            "lr": 0.0001,
            "weight_decay": 0.0001
        },
        "clip_grad": {
            "max_norm": 0.1,
            "norm_type": 2
        },
        "paramwise_cfg": {
            "custom_keys": {
                "backbone": {
                    "lr_mult": 0.1
                }
            }
        }
    },
    "max_epochs": 15,
    "train_cfg": {
        "type": "EpochBasedTrainLoop",
        "max_epochs": 15,
        "val_interval": 1
    },
    "val_cfg": {
        "type": "ValLoop"
    },
    "test_cfg": {
        "type": "TestLoop"
    },
    "param_scheduler": [
        {
            "type": "MultiStepLR",
            "begin": 0,
            "end": 12,
            "by_epoch": true,
            "milestones": [
                11
            ],
            "gamma": 0.1
        }
    ],
    "pretrained": "https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth",
    "num_levels": 5,
    "seed": 2022,
    "gpu_ids": [
        0
    ],
    "work_dir": "/work_dirs/dino-5scale_swin-l_8xb2-12e_coco_trash/fold_0",
    "device": "cuda"
}