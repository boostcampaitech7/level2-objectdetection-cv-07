{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.ultralytics.com/datasets/detect/#ultralytics-yolo-format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "def copy_images(coco_json_path, src_image_dir, dest_image_dir):\n",
    "    with open(coco_json_path, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    images = coco_data['images']\n",
    "\n",
    "    for image_info in images:\n",
    "        file_name = image_info['file_name']\n",
    "        src_file_path = os.path.join(src_image_dir, file_name)\n",
    "        dest_file_path = os.path.join(dest_image_dir, os.path.basename(file_name))\n",
    "\n",
    "        os.makedirs(os.path.dirname(dest_file_path), exist_ok=True)\n",
    "\n",
    "        if os.path.exists(src_file_path):\n",
    "            shutil.copy(src_file_path, dest_file_path)\n",
    "        else:\n",
    "            print(f\"File {src_file_path} does not exist.\")\n",
    "\n",
    "# Paths\n",
    "train_coco_json = '/data/ephemeral/home/data/dataset/train_0_5.json'\n",
    "val_coco_json = '/data/ephemeral/home/data/dataset/valid_0_5.json'\n",
    "\n",
    "src_image_dir = '/data/ephemeral/home/data/dataset'\n",
    "train_dest_image_dir = '/data/ephemeral/home/data/images/train'\n",
    "val_dest_image_dir = '/data/ephemeral/home/data/images/valid'\n",
    "\n",
    "# Copy images\n",
    "copy_images(train_coco_json, src_image_dir, train_dest_image_dir)\n",
    "copy_images(val_coco_json, src_image_dir, val_dest_image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels 만들기\n",
    "import os\n",
    "import json\n",
    "\n",
    "def convert_coco_to_yolo(coco_json_path, output_dir, image_dir):\n",
    "    with open(coco_json_path, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    images = {img['id']: img for img in coco_data['images']}\n",
    "    annotations = coco_data['annotations']\n",
    "    categories = {cat['id']: cat['name'] for cat in coco_data['categories']}\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for ann in annotations:\n",
    "        image_id = ann['image_id']\n",
    "        image_info = images[image_id]\n",
    "        file_name = image_info['file_name']\n",
    "        width = image_info['width']\n",
    "        height = image_info['height']\n",
    "\n",
    "        bbox = ann['bbox']\n",
    "        x_center = (bbox[0] + bbox[2] / 2) / width\n",
    "        y_center = (bbox[1] + bbox[3] / 2) / height\n",
    "        w = bbox[2] / width\n",
    "        h = bbox[3] / height\n",
    "\n",
    "        category_id = ann['category_id']\n",
    "        yolo_annotation = f\"{category_id} {x_center} {y_center} {w} {h}\\n\"\n",
    "\n",
    "        label_file_name = os.path.splitext(file_name)[0].replace('train/', '') + '.txt'\n",
    "        label_file_path = os.path.join(output_dir, label_file_name)\n",
    "\n",
    "        with open(label_file_path, 'a') as label_file:\n",
    "            label_file.write(yolo_annotation)\n",
    "\n",
    "# # Paths\n",
    "# train_coco_json = '/data/ephemeral/home/data/images/train_0_5.json'\n",
    "# val_coco_json = '/data/ephemeral/home/data/images/valid_0_5.json'\n",
    "\n",
    "# train_image_dir = '/data/ephemeral/home/data/images/train'\n",
    "# val_image_dir = '/data/ephemeral/home/data/images/train'\n",
    "\n",
    "# train_output_dir = '/data/ephemeral/home/data/labels/train'\n",
    "# val_output_dir = '/data/ephemeral/home/data/labels/val'\n",
    "\n",
    "# # Ensure output directories exist\n",
    "# os.makedirs(train_output_dir, exist_ok=True)\n",
    "# os.makedirs(val_output_dir, exist_ok=True)\n",
    "\n",
    "# # Convert COCO to YOLO\n",
    "# convert_coco_to_yolo(train_coco_json, train_output_dir, train_image_dir)\n",
    "# convert_coco_to_yolo(val_coco_json, val_output_dir, val_image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "test_coco_json = '/data/ephemeral/home/data/images/test.json'\n",
    "\n",
    "test_image_dir = '/data/ephemeral/home/data/images/test'\n",
    "\n",
    "test_output_dir = '/data/ephemeral/home/data/labels/test'\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(test_output_dir, exist_ok=True)\n",
    "\n",
    "# Convert COCO to YOLO\n",
    "convert_coco_to_yolo(test_coco_json, test_output_dir, test_image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pycocotools.coco import COCO\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from ultralytics import YOLO\n",
    "from dotenv import load_dotenv\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /data/ephemeral/home/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msoojeoong43\u001b[0m (\u001b[33mluckyvicky\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/home/Sojeong/level2-objectdetection-cv-07/Backbone/Sojeong/1011_ultralytics/wandb/run-20241011_160158-z16eomdi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/luckyvicky/object-detection/runs/z16eomdi' target=\"_blank\">yolov8</a></strong> to <a href='https://wandb.ai/luckyvicky/object-detection' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/luckyvicky/object-detection' target=\"_blank\">https://wandb.ai/luckyvicky/object-detection</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/luckyvicky/object-detection/runs/z16eomdi' target=\"_blank\">https://wandb.ai/luckyvicky/object-detection/runs/z16eomdi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.9 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.238 🚀 Python-3.10.13 torch-1.12.1+cu116 CUDA:0 (Tesla V100-SXM2-32GB, 32501MiB)\n",
      "WARNING ⚠️ Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8x.pt, data=coco_dataset.yaml, epochs=100, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=0, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n",
      "Overriding model.yaml nc=80 with nc=10\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      2320  ultralytics.nn.modules.conv.Conv             [3, 80, 3, 2]                 \n",
      "  1                  -1  1    115520  ultralytics.nn.modules.conv.Conv             [80, 160, 3, 2]               \n",
      "  2                  -1  3    436800  ultralytics.nn.modules.block.C2f             [160, 160, 3, True]           \n",
      "  3                  -1  1    461440  ultralytics.nn.modules.conv.Conv             [160, 320, 3, 2]              \n",
      "  4                  -1  6   3281920  ultralytics.nn.modules.block.C2f             [320, 320, 6, True]           \n",
      "  5                  -1  1   1844480  ultralytics.nn.modules.conv.Conv             [320, 640, 3, 2]              \n",
      "  6                  -1  6  13117440  ultralytics.nn.modules.block.C2f             [640, 640, 6, True]           \n",
      "  7                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n",
      "  8                  -1  3   6969600  ultralytics.nn.modules.block.C2f             [640, 640, 3, True]           \n",
      "  9                  -1  1   1025920  ultralytics.nn.modules.block.SPPF            [640, 640, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  3   1948800  ultralytics.nn.modules.block.C2f             [960, 320, 3]                 \n",
      " 16                  -1  1    922240  ultralytics.nn.modules.conv.Conv             [320, 320, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  3   7174400  ultralytics.nn.modules.block.C2f             [960, 640, 3]                 \n",
      " 19                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n",
      " 22        [15, 18, 21]  1   8727598  ultralytics.nn.modules.head.Detect           [10, [320, 640, 640]]         \n",
      "Model summary: 365 layers, 68162238 parameters, 68162222 gradients, 258.2 GFLOPs\n",
      "\n",
      "Transferred 589/595 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to 'yolov8n.pt'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2932cafaa37d4b498335adccea5f0e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/6.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /data/ephemeral/home/data/labels/train... 3943 images, 0 backgrounds, 0 corrupt: 100%|██████████| 3943/3943 [00:04<00:00, 864.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /data/ephemeral/home/data/images/train/4041.jpg: 1 duplicate labels removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /data/ephemeral/home/data/labels/train.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /data/ephemeral/home/data/labels/valid... 0 images, 940 backgrounds, 0 corrupt: 100%|██████████| 940/940 [00:00<00:00, 1155.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ No labels found in /data/ephemeral/home/data/labels/valid.cache. See https://docs.ultralytics.com/datasets/detect for dataset formatting guidance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /data/ephemeral/home/data/labels/valid.cache\n",
      "WARNING ⚠️ No labels found in /data/ephemeral/home/data/labels/valid.cache, training may not work correctly. See https://docs.ultralytics.com/datasets/detect for dataset formatting guidance.\n",
      "Plotting labels to runs/detect/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 97 weight(decay=0.0), 104 weight(decay=0.0005), 103 bias(decay=0.0)\n",
      "100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      1/100      12.9G     0.9631      4.181      1.441        130        640:   3%|▎         | 8/247 [00:07<03:32,  1.13it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m model\u001b[38;5;241m.\u001b[39madd_callback(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_train_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m, wandb\u001b[38;5;241m.\u001b[39mlog)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m train_results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcoco_dataset.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# path to COCO dataset YAML\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# number of training epochs\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# training image size\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# device to run on, i.e. device=0 or device=0,1,2,3 or device=cpu\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Finish the W&B run\u001b[39;00m\n\u001b[1;32m     34\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/model.py:356\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 356\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/trainer.py:189\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/trainer.py:357\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ni \u001b[38;5;241m-\u001b[39m last_opt_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulate:\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m     last_opt_step \u001b[38;5;241m=\u001b[39m ni\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;66;03m# Timed stopping\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/trainer.py:491\u001b[0m, in \u001b[0;36mBaseTrainer.optimizer_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39munscale_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer)  \u001b[38;5;66;03m# unscale gradients\u001b[39;00m\n\u001b[1;32m    490\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10.0\u001b[39m)  \u001b[38;5;66;03m# clip gradients\u001b[39;00m\n\u001b[0;32m--> 491\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:338\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 338\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:284\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    283\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    285\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:284\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    283\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    285\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ultralytics import YOLO\n",
    "from dotenv import load_dotenv\n",
    "import wandb\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# W&B login\n",
    "wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "if wandb_api_key:\n",
    "    wandb.login(key=wandb_api_key)\n",
    "else:\n",
    "    raise ValueError(\"WANDB_API_KEY not found in .env file\")\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolov8x.pt\")\n",
    "\n",
    "# Initialize WandB\n",
    "wandb.init(project=\"object-detection\", entity=\"luckyvicky\", name=\"yolov8\")\n",
    "\n",
    "# Add W&B callback for Ultralytics\n",
    "model.add_callback(\"on_train_epoch_end\", wandb.log)\n",
    "\n",
    "# Train the model\n",
    "train_results = model.train(\n",
    "    data=\"coco_dataset.yaml\",  # path to COCO dataset YAML\n",
    "    epochs=100,  # number of training epochs\n",
    "    imgsz=640,  # training image size\n",
    "    device=0,  # device to run on, i.e. device=0 or device=0,1,2,3 or device=cpu\n",
    ")\n",
    "\n",
    "# Finish the W&B run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, annotation, data_dir, transforms=None, image_size=512):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.coco = COCO(annotation)\n",
    "        self.image_size = image_size\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "        image = cv2.imread(os.path.join(self.data_dir, image_info['file_name']))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_info['id'])\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        boxes = np.array([x['bbox'] for x in anns])\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "        boxes[:, 0] /= int(self.image_size)\n",
    "        boxes[:, 1] /= int(self.image_size)\n",
    "        boxes[:, 2] /= int(self.image_size)\n",
    "        boxes[:, 3] /= int(self.image_size)\n",
    "\n",
    "        labels = list([x['category_id'] for x in anns])\n",
    "        areas = np.array([x['area'] for x in anns])\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "\n",
    "        is_crowds = np.array([x['iscrowd'] for x in anns])\n",
    "        is_crowds = torch.as_tensor(is_crowds, dtype=torch.int64)\n",
    "\n",
    "        target = {'boxes': boxes, 'labels': labels, 'image_id': torch.tensor([index]), 'area': areas,\n",
    "                  'iscrowd': is_crowds}\n",
    "\n",
    "        if self.transforms:\n",
    "            sample = {\n",
    "                'image': image,\n",
    "                'bboxes': target['boxes'],\n",
    "                'labels': labels\n",
    "            }\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "            target['boxes'] = [list(box) for box in sample['bboxes']]\n",
    "\n",
    "        labels_out = torch.zeros((len(labels), 6))\n",
    "        for i in range(len(labels)):\n",
    "            labels_out[i, 1:] = torch.tensor(target['boxes'][i])\n",
    "\n",
    "        return image, labels_out, image_id\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.coco.getImgIds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.Resize(512, 512),\n",
    "        A.Flip(p=0.5),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "def get_valid_transform():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.28s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/data/ephemeral/home/data/dataset/train'\n",
    "annotation_train = '/data/ephemeral/home/data/dataset/train_0_5.json'\n",
    "annotation_val = '/data/ephemeral/home/data/dataset/valid_0_5.json'\n",
    "\n",
    "train_dataset = CustomDataset(annotation_train, data_dir, get_train_transform())\n",
    "val_dataset = CustomDataset(annotation_val, data_dir, get_valid_transform())\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=lambda x: tuple(zip(*x))\n",
    ")\n",
    "\n",
    "val_data_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=lambda x: tuple(zip(*x))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade ultralytics==8.0.238 wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/home/Sojeong/level2-objectdetection-cv-07/Backbone/Sojeong/1011_ultralytics/wandb/run-20241011_151806-837au95z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/luckyvicky/object-detection/runs/837au95z' target=\"_blank\">yolo</a></strong> to <a href='https://wandb.ai/luckyvicky/object-detection' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/luckyvicky/object-detection' target=\"_blank\">https://wandb.ai/luckyvicky/object-detection</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/luckyvicky/object-detection/runs/837au95z' target=\"_blank\">https://wandb.ai/luckyvicky/object-detection/runs/837au95z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.9 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.238 🚀 Python-3.10.13 torch-1.12.1+cu116 CUDA:0 (Tesla V100-SXM2-32GB, 32501MiB)\n",
      "WARNING ⚠️ Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8x.pt, data=/data/ephemeral/home/Sojeong/level2-objectdetection-cv-07/Backbone/Sojeong/1011_ultralytics/coco_dataset.yaml, epochs=100, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=0, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train2\n",
      "Overriding model.yaml nc=80 with nc=10\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      2320  ultralytics.nn.modules.conv.Conv             [3, 80, 3, 2]                 \n",
      "  1                  -1  1    115520  ultralytics.nn.modules.conv.Conv             [80, 160, 3, 2]               \n",
      "  2                  -1  3    436800  ultralytics.nn.modules.block.C2f             [160, 160, 3, True]           \n",
      "  3                  -1  1    461440  ultralytics.nn.modules.conv.Conv             [160, 320, 3, 2]              \n",
      "  4                  -1  6   3281920  ultralytics.nn.modules.block.C2f             [320, 320, 6, True]           \n",
      "  5                  -1  1   1844480  ultralytics.nn.modules.conv.Conv             [320, 640, 3, 2]              \n",
      "  6                  -1  6  13117440  ultralytics.nn.modules.block.C2f             [640, 640, 6, True]           \n",
      "  7                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n",
      "  8                  -1  3   6969600  ultralytics.nn.modules.block.C2f             [640, 640, 3, True]           \n",
      "  9                  -1  1   1025920  ultralytics.nn.modules.block.SPPF            [640, 640, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  3   1948800  ultralytics.nn.modules.block.C2f             [960, 320, 3]                 \n",
      " 16                  -1  1    922240  ultralytics.nn.modules.conv.Conv             [320, 320, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  3   7174400  ultralytics.nn.modules.block.C2f             [960, 640, 3]                 \n",
      " 19                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n",
      " 22        [15, 18, 21]  1   8727598  ultralytics.nn.modules.head.Detect           [10, [320, 640, 640]]         \n",
      "Model summary: 365 layers, 68162238 parameters, 68162222 gradients, 258.2 GFLOPs\n",
      "\n",
      "Transferred 589/595 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train2', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /data/ephemeral/home/data/dataset/train... 0 images, 4883 backgrounds, 0 corrupt: 100%|██████████| 4883/4883 [00:02<00:00, 2093.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ No labels found in /data/ephemeral/home/data/dataset/train.cache. See https://docs.ultralytics.com/datasets/detect for dataset formatting guidance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /data/ephemeral/home/data/dataset/train.cache\n",
      "WARNING ⚠️ No labels found in /data/ephemeral/home/data/dataset/train.cache, training may not work correctly. See https://docs.ultralytics.com/datasets/detect for dataset formatting guidance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /data/ephemeral/home/data/dataset/train.cache... 0 images, 4883 backgrounds, 0 corrupt: 100%|██████████| 4883/4883 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ No labels found in /data/ephemeral/home/data/dataset/train.cache, training may not work correctly. See https://docs.ultralytics.com/datasets/detect for dataset formatting guidance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train2/labels.jpg... \n",
      "zero-size array to reduction operation maximum which has no identity\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 97 weight(decay=0.0), 104 weight(decay=0.0005), 103 bias(decay=0.0)\n",
      "100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      1/100      12.8G          0      236.8          0          0        640:   5%|▍         | 15/306 [00:11<03:40,  1.32it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39madd_callback(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_train_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m, wandb\u001b[38;5;241m.\u001b[39mlog)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 모델 학습\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m train_results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/data/ephemeral/home/Sojeong/level2-objectdetection-cv-07/Backbone/Sojeong/1011_ultralytics/coco_dataset.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# path to COCO dataset YAML\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# number of training epochs\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# training image size\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# device to run on, i.e. device=0 or device=0,1,2,3 or device=cpu\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Finish the W&B run\u001b[39;00m\n\u001b[1;32m     21\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/model.py:356\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 356\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/trainer.py:189\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/trainer.py:357\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ni \u001b[38;5;241m-\u001b[39m last_opt_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulate:\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m     last_opt_step \u001b[38;5;241m=\u001b[39m ni\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;66;03m# Timed stopping\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/trainer.py:491\u001b[0m, in \u001b[0;36mBaseTrainer.optimizer_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39munscale_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer)  \u001b[38;5;66;03m# unscale gradients\u001b[39;00m\n\u001b[1;32m    490\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10.0\u001b[39m)  \u001b[38;5;66;03m# clip gradients\u001b[39;00m\n\u001b[0;32m--> 491\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:338\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 338\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:284\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    283\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    285\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:284\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    283\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    285\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO  # YOLOv11 모델 임포트\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolov8x.pt\")\n",
    "\n",
    "# Initialize WandB\n",
    "wandb.init(project=\"object-detection\", entity=\"luckyvicky\", name=\"yolo\")\n",
    "\n",
    "# Add W&B callback for Ultralytics\n",
    "model.add_callback(\"on_train_epoch_end\", wandb.log)\n",
    "\n",
    "# 모델 학습\n",
    "train_results = model.train(\n",
    "    data=\"/data/ephemeral/home/Sojeong/level2-objectdetection-cv-07/Backbone/Sojeong/1011_ultralytics/coco_dataset.yaml\",  # path to COCO dataset YAML\n",
    "    epochs=100,  # number of training epochs\n",
    "    imgsz=640,  # training image size\n",
    "    device=0,  # device to run on, i.e. device=0 or device=0,1,2,3 or device=cpu\n",
    ")\n",
    "\n",
    "# Finish the W&B run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate model performance on the validation set\n",
    "# metrics = model.val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform object detection on an image\n",
    "# results = model(\"path/to/image.jpg\")\n",
    "# results[0].show()\n",
    "\n",
    "# # Export the model to ONNX format\n",
    "# #path = model.export(format=\"onnx\")  # return path to exported model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
