[10/18 12:56:11] detectron2 INFO: Rank of current process: 0. World size: 1
[10/18 12:56:11] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]
numpy                            1.22.4
detectron2                       0.6 @/opt/conda/lib/python3.10/site-packages/detectron2
detectron2._C                    not built correctly: /opt/conda/lib/python3.10/site-packages/detectron2/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN2at4_ops7reshape4callERKNS_6TensorEN3c108ArrayRefINS5_6SymIntEEE
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
detectron2 arch flags            /opt/conda/lib/python3.10/site-packages/detectron2/_C.cpython-310-x86_64-linux-gnu.so
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.12.1+cu116 @/opt/conda/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            Tesla V100-SXM2-32GB (arch=7.0)
Driver version                   535.161.08
CUDA_HOME                        None - invalid!
Pillow                           10.4.0
torchvision                      0.13.1+cu116 @/opt/conda/lib/python3.10/site-packages/torchvision
torchvision arch flags           /opt/conda/lib/python3.10/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.8.1
-------------------------------  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/18 12:56:11] detectron2 INFO: Command line arguments: Namespace(config_file='ViTDet/configs/COCO/cascade_mask_rcnn_swin_l_in21k_50ep.py', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:49152', opts=[])
[10/18 12:56:11] detectron2 INFO: Contents of args.config_file=ViTDet/configs/COCO/cascade_mask_rcnn_swin_l_in21k_50ep.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcascade_mask_rcnn_swin_b_in21k_50ep[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15m([39m
[38;5;15m    [39m[38;5;15mdataloader[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mlr_multiplier[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mmodel[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mtrain[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15moptimizer[39m[38;5;15m,[39m
[38;5;15m)[39m


[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmapper[39m[38;5;197m.[39m[38;5;15muse_instance_mask[39m[38;5;197m=[39m[38;5;81mFalse[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmapper[39m[38;5;197m.[39m[38;5;15mrecompute_boxes[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mFalse[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtotal_batch_size[39m[38;5;197m=[39m[38;5;141m2[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdataset[39m[38;5;197m.[39m[38;5;15mnames[39m[38;5;197m=[39m[38;5;186m'[39m[38;5;186mcoco_trash_train[39m[38;5;186m'[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtest[39m[38;5;197m.[39m[38;5;15mdataset[39m[38;5;197m.[39m[38;5;15mnames[39m[38;5;197m=[39m[38;5;186m'[39m[38;5;186mcoco_trash_test[39m[38;5;186m'[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mevaluator[39m[38;5;197m.[39m[38;5;15mdataset_name[39m[38;5;197m=[39m[38;5;186m'[39m[38;5;186mcoco_trash_test[39m[38;5;186m'[39m


[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mdepths[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m[[39m[38;5;141m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m18[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m2[39m[38;5;15m][39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mdrop_path_rate[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15membed_dim[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m192[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mnum_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m[[39m[38;5;141m6[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m12[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m24[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m48[39m[38;5;15m][39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mroi_heads[39m[38;5;197m.[39m[38;5;15mnum_classes[39m[38;5;197m=[39m[38;5;141m10[39m

[38;5;81mdel[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mroi_heads[39m[38;5;15m[[39m[38;5;186m'[39m[38;5;186mmask_in_features[39m[38;5;186m'[39m[38;5;15m][39m
[38;5;81mdel[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mroi_heads[39m[38;5;15m[[39m[38;5;186m'[39m[38;5;186mmask_pooler[39m[38;5;186m'[39m[38;5;15m][39m
[38;5;81mdel[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mroi_heads[39m[38;5;15m[[39m[38;5;186m'[39m[38;5;186mmask_head[39m[38;5;186m'[39m[38;5;15m][39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15meval_period[39m[38;5;197m=[39m[38;5;141m5000[39m
[38;5;242m#train.max_iter[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15minit_checkpoint[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://ImageNetPretrained/swin/swin_large_patch4_window7_224_22k.pth[39m[38;5;186m"[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15moutput_dir[39m[38;5;197m=[39m[38;5;186m'[39m[38;5;186m/data/ephemeral/home/Seungcheol/level2-objectdetection-cv-07/swin_rcnn/output[39m[38;5;186m'[39m

[38;5;15moptimizer[39m[38;5;197m.[39m[38;5;15mlr[39m[38;5;197m=[39m[38;5;141m0.0001[39m

[10/18 12:56:12] detectron2 INFO: Full config saved to /data/ephemeral/home/Seungcheol/level2-objectdetection-cv-07/swin_rcnn/output/config.yaml
[10/18 12:56:12] d2.utils.env INFO: Using a generated random seed 12136851
[10/18 12:56:16] detectron2 INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(
      192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
      (norm): LayerNorm()
    )
    (fpn_output2): Conv2d(
      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
      (norm): LayerNorm()
    )
    (fpn_lateral3): Conv2d(
      384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
      (norm): LayerNorm()
    )
    (fpn_output3): Conv2d(
      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
      (norm): LayerNorm()
    )
    (fpn_lateral4): Conv2d(
      768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
      (norm): LayerNorm()
    )
    (fpn_output4): Conv2d(
      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
      (norm): LayerNorm()
    )
    (fpn_lateral5): Conv2d(
      1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
      (norm): LayerNorm()
    )
    (fpn_output5): Conv2d(
      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
      (norm): LayerNorm()
    )
    (top_block): LastLevelMaxPool()
    (bottom_up): SwinTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=192, out_features=576, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=192, out_features=192, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): Identity()
              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=192, out_features=768, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=768, out_features=192, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock(
              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=192, out_features=576, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=192, out_features=192, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.017)
              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=192, out_features=768, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=768, out_features=192, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (downsample): PatchMerging(
            (reduction): Linear(in_features=768, out_features=384, bias=False)
            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.035)
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.052)
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (downsample): PatchMerging(
            (reduction): Linear(in_features=1536, out_features=768, bias=False)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.070)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.087)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (2): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.104)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (3): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.122)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (4): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.139)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (5): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.157)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (6): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.174)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (7): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.191)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (8): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.209)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (9): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.226)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (10): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.243)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (11): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.261)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (12): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.278)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (13): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.296)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (14): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.313)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (15): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.330)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (16): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.348)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (17): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.365)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (downsample): PatchMerging(
            (reduction): Linear(in_features=3072, out_features=1536, bias=False)
            (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=1536, out_features=4608, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=1536, out_features=1536, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.383)
              (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock(
              (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=1536, out_features=4608, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=1536, out_features=1536, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.400)
              (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
        )
      )
      (norm0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Sequential(
        (conv0): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): CascadeROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): ModuleList(
      (0): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (1): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (2): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
    )
    (box_predictor): ModuleList(
      (0): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=11, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (1): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=11, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (2): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=11, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
  )
)
[10/18 12:56:18] d2.data.datasets.coco INFO: Loaded 4437 images in COCO format from /data/ephemeral/home/Seungcheol/level2-objectdetection-cv-07/Split_data/train_0_10.json
[10/18 12:56:18] d2.data.build INFO: Removed 0 images with no usable annotations. 4437 images left.
[10/18 12:56:18] d2.data.build INFO: Distribution of instances among all 10 categories:
[36m|   category    | #instances   |  category   | #instances   |  category  | #instances   |
|:-------------:|:-------------|:-----------:|:-------------|:----------:|:-------------|
| General trash | 3523         |    Paper    | 5785         | Paper pack | 840          |
|     Metal     | 882          |    Glass    | 891          |  Plastic   | 2707         |
|   Styrofoam   | 1108         | Plastic bag | 4664         |  Battery   | 146          |
|   Clothing    | 412          |             |              |            |              |
|     total     | 20958        |             |              |            |              |[0m
[10/18 12:56:18] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [RandomFlip(), ResizeScale(min_scale=0.1, max_scale=2.0, target_height=1024, target_width=1024), FixedSizeCrop(crop_size=[1024, 1024], pad=False)]
[10/18 12:56:18] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/18 12:56:18] d2.data.common INFO: Serializing 4437 elements to byte tensors and concatenating them all ...
[10/18 12:56:18] d2.data.common INFO: Serialized dataset takes 2.07 MiB
[10/18 12:56:18] d2.data.build INFO: Making batched data loader with batch_size=2
[10/18 12:56:18] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from detectron2://ImageNetPretrained/swin/swin_large_patch4_window7_224_22k.pth ...
[10/18 12:56:18] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /data/ephemeral/home/.torch/iopath_cache/detectron2/ImageNetPretrained/swin/swin_large_patch4_window7_224_22k.pth ...
[10/18 12:56:19] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.bottom_up.layers.0.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.0.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.0.blocks.0.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.0.blocks.0.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.0.blocks.0.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.0.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.0.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.0.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.0.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.0.blocks.1.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.0.blocks.1.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.0.blocks.1.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.0.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.0.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.0.downsample.norm.{bias, weight}[0m
[34mbackbone.bottom_up.layers.0.downsample.reduction.weight[0m
[34mbackbone.bottom_up.layers.1.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.1.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.1.blocks.0.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.1.blocks.0.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.1.blocks.0.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.1.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.1.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.1.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.1.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.1.blocks.1.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.1.blocks.1.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.1.blocks.1.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.1.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.1.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.1.downsample.norm.{bias, weight}[0m
[34mbackbone.bottom_up.layers.1.downsample.reduction.weight[0m
[34mbackbone.bottom_up.layers.2.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.0.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.0.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.0.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.1.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.1.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.1.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.10.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.10.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.10.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.11.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.11.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.11.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.12.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.12.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.12.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.13.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.13.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.13.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.14.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.14.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.14.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.15.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.15.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.15.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.16.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.16.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.16.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.17.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.17.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.17.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.2.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.2.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.2.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.3.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.3.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.3.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.4.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.4.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.4.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.5.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.5.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.5.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.6.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.6.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.6.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.7.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.7.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.7.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.8.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.8.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.8.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.9.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.9.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.9.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.downsample.norm.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.downsample.reduction.weight[0m
[34mbackbone.bottom_up.layers.3.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.3.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.3.blocks.0.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.3.blocks.0.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.3.blocks.0.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.3.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.3.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.3.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.3.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.3.blocks.1.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.3.blocks.1.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.3.blocks.1.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.3.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.3.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.norm0.{bias, weight}[0m
[34mbackbone.bottom_up.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.norm3.{bias, weight}[0m
[34mbackbone.bottom_up.patch_embed.norm.{bias, weight}[0m
[34mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
[34mbackbone.fpn_lateral2.norm.{bias, weight}[0m
[34mbackbone.fpn_lateral2.weight[0m
[34mbackbone.fpn_lateral3.norm.{bias, weight}[0m
[34mbackbone.fpn_lateral3.weight[0m
[34mbackbone.fpn_lateral4.norm.{bias, weight}[0m
[34mbackbone.fpn_lateral4.weight[0m
[34mbackbone.fpn_lateral5.norm.{bias, weight}[0m
[34mbackbone.fpn_lateral5.weight[0m
[34mbackbone.fpn_output2.norm.{bias, weight}[0m
[34mbackbone.fpn_output2.weight[0m
[34mbackbone.fpn_output3.norm.{bias, weight}[0m
[34mbackbone.fpn_output3.weight[0m
[34mbackbone.fpn_output4.norm.{bias, weight}[0m
[34mbackbone.fpn_output4.weight[0m
[34mbackbone.fpn_output5.norm.{bias, weight}[0m
[34mbackbone.fpn_output5.weight[0m
[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.conv0.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.conv1.{bias, weight}[0m
[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
[34mroi_heads.box_head.0.conv1.norm.{bias, weight}[0m
[34mroi_heads.box_head.0.conv1.weight[0m
[34mroi_heads.box_head.0.conv2.norm.{bias, weight}[0m
[34mroi_heads.box_head.0.conv2.weight[0m
[34mroi_heads.box_head.0.conv3.norm.{bias, weight}[0m
[34mroi_heads.box_head.0.conv3.weight[0m
[34mroi_heads.box_head.0.conv4.norm.{bias, weight}[0m
[34mroi_heads.box_head.0.conv4.weight[0m
[34mroi_heads.box_head.0.fc1.{bias, weight}[0m
[34mroi_heads.box_head.1.conv1.norm.{bias, weight}[0m
[34mroi_heads.box_head.1.conv1.weight[0m
[34mroi_heads.box_head.1.conv2.norm.{bias, weight}[0m
[34mroi_heads.box_head.1.conv2.weight[0m
[34mroi_heads.box_head.1.conv3.norm.{bias, weight}[0m
[34mroi_heads.box_head.1.conv3.weight[0m
[34mroi_heads.box_head.1.conv4.norm.{bias, weight}[0m
[34mroi_heads.box_head.1.conv4.weight[0m
[34mroi_heads.box_head.1.fc1.{bias, weight}[0m
[34mroi_heads.box_head.2.conv1.norm.{bias, weight}[0m
[34mroi_heads.box_head.2.conv1.weight[0m
[34mroi_heads.box_head.2.conv2.norm.{bias, weight}[0m
[34mroi_heads.box_head.2.conv2.weight[0m
[34mroi_heads.box_head.2.conv3.norm.{bias, weight}[0m
[34mroi_heads.box_head.2.conv3.weight[0m
[34mroi_heads.box_head.2.conv4.norm.{bias, weight}[0m
[34mroi_heads.box_head.2.conv4.weight[0m
[34mroi_heads.box_head.2.fc1.{bias, weight}[0m
[34mroi_heads.box_predictor.0.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.0.cls_score.{bias, weight}[0m
[34mroi_heads.box_predictor.1.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.1.cls_score.{bias, weight}[0m
[34mroi_heads.box_predictor.2.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.2.cls_score.{bias, weight}[0m
[10/18 12:56:19] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mpatch_embed.proj.{bias, weight}[0m
  [35mpatch_embed.norm.{bias, weight}[0m
  [35mlayers.0.blocks.0.norm1.{bias, weight}[0m
  [35mlayers.0.blocks.0.attn.qkv.{bias, weight}[0m
  [35mlayers.0.blocks.0.attn.proj.{bias, weight}[0m
  [35mlayers.0.blocks.0.norm2.{bias, weight}[0m
  [35mlayers.0.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mlayers.0.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mlayers.0.blocks.1.norm1.{bias, weight}[0m
  [35mlayers.0.blocks.1.attn.qkv.{bias, weight}[0m
  [35mlayers.0.blocks.1.attn.proj.{bias, weight}[0m
  [35mlayers.0.blocks.1.norm2.{bias, weight}[0m
  [35mlayers.0.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mlayers.0.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mlayers.0.downsample.norm.{bias, weight}[0m
  [35mlayers.1.blocks.0.norm1.{bias, weight}[0m
  [35mlayers.1.blocks.0.attn.qkv.{bias, weight}[0m
  [35mlayers.1.blocks.0.attn.proj.{bias, weight}[0m
  [35mlayers.1.blocks.0.norm2.{bias, weight}[0m
  [35mlayers.1.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mlayers.1.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mlayers.1.blocks.1.norm1.{bias, weight}[0m
  [35mlayers.1.blocks.1.attn.qkv.{bias, weight}[0m
  [35mlayers.1.blocks.1.attn.proj.{bias, weight}[0m
  [35mlayers.1.blocks.1.norm2.{bias, weight}[0m
  [35mlayers.1.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mlayers.1.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mlayers.1.downsample.norm.{bias, weight}[0m
  [35mlayers.2.blocks.0.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.0.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.0.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.0.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.blocks.1.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.1.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.1.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.1.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.blocks.2.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.2.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.2.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.2.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.blocks.3.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.3.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.3.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.3.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.blocks.4.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.4.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.4.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.4.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.blocks.5.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.5.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.5.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.5.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.blocks.6.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.6.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.6.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.6.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.blocks.7.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.7.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.7.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.7.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.blocks.8.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.8.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.8.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.8.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.blocks.9.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.9.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.9.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.9.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.blocks.10.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.10.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.10.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.10.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.blocks.11.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.11.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.11.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.11.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.11.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.blocks.12.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.12.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.12.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.12.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.12.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.12.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.blocks.13.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.13.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.13.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.13.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.13.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.13.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.blocks.14.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.14.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.14.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.14.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.14.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.14.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.blocks.15.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.15.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.15.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.15.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.15.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.15.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.blocks.16.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.16.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.16.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.16.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.16.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.16.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.blocks.17.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.17.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.17.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.17.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.17.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.17.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.downsample.norm.{bias, weight}[0m
  [35mlayers.3.blocks.0.norm1.{bias, weight}[0m
  [35mlayers.3.blocks.0.attn.qkv.{bias, weight}[0m
  [35mlayers.3.blocks.0.attn.proj.{bias, weight}[0m
  [35mlayers.3.blocks.0.norm2.{bias, weight}[0m
  [35mlayers.3.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mlayers.3.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mlayers.3.blocks.1.norm1.{bias, weight}[0m
  [35mlayers.3.blocks.1.attn.qkv.{bias, weight}[0m
  [35mlayers.3.blocks.1.attn.proj.{bias, weight}[0m
  [35mlayers.3.blocks.1.norm2.{bias, weight}[0m
  [35mlayers.3.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mlayers.3.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mnorm.{bias, weight}[0m
  [35mhead.{bias, weight}[0m
  [35mlayers.0.blocks.0.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.0.blocks.1.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.1.blocks.0.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.1.blocks.1.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.0.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.1.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.2.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.3.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.4.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.5.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.6.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.7.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.8.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.9.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.10.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.11.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.12.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.13.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.14.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.15.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.16.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.17.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.3.blocks.0.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.3.blocks.1.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.0.blocks.1.attn_mask[0m
  [35mlayers.1.blocks.1.attn_mask[0m
  [35mlayers.2.blocks.1.attn_mask[0m
  [35mlayers.2.blocks.3.attn_mask[0m
  [35mlayers.2.blocks.5.attn_mask[0m
  [35mlayers.2.blocks.7.attn_mask[0m
  [35mlayers.2.blocks.9.attn_mask[0m
  [35mlayers.2.blocks.11.attn_mask[0m
  [35mlayers.2.blocks.13.attn_mask[0m
  [35mlayers.2.blocks.15.attn_mask[0m
  [35mlayers.2.blocks.17.attn_mask[0m
  [35mlayers.0.downsample.reduction.weight[0m
  [35mlayers.1.downsample.reduction.weight[0m
  [35mlayers.2.downsample.reduction.weight[0m
[10/18 12:56:19] d2.engine.train_loop INFO: Starting training from iteration 0
[10/18 12:56:33] d2.utils.events INFO:  eta: 15:54:01  iter: 19  total_loss: 6.238  loss_cls_stage0: 1.746  loss_box_reg_stage0: 0.03694  loss_cls_stage1: 1.77  loss_box_reg_stage1: 0.01721  loss_cls_stage2: 1.773  loss_box_reg_stage2: 0.004728  loss_rpn_cls: 0.6957  loss_rpn_loc: 0.02464    time: 0.6167  last_time: 0.6229  data_time: 0.0174  last_data_time: 0.0046   lr: 1.5285e-05  max_mem: 17788M
[10/18 12:56:46] d2.utils.events INFO:  eta: 15:59:24  iter: 39  total_loss: 1.464  loss_cls_stage0: 0.1965  loss_box_reg_stage0: 0.09163  loss_cls_stage1: 0.1548  loss_box_reg_stage1: 0.05902  loss_cls_stage2: 0.1183  loss_box_reg_stage2: 0.01252  loss_rpn_cls: 0.637  loss_rpn_loc: 0.0689    time: 0.6240  last_time: 0.6290  data_time: 0.0046  last_data_time: 0.0040   lr: 3.1269e-05  max_mem: 17788M
[10/18 12:56:58] d2.utils.events INFO:  eta: 16:03:59  iter: 59  total_loss: 1.153  loss_cls_stage0: 0.2435  loss_box_reg_stage0: 0.12  loss_cls_stage1: 0.1406  loss_box_reg_stage1: 0.07483  loss_cls_stage2: 0.09005  loss_box_reg_stage2: 0.02645  loss_rpn_cls: 0.3378  loss_rpn_loc: 0.07437    time: 0.6242  last_time: 0.6278  data_time: 0.0045  last_data_time: 0.0039   lr: 4.7253e-05  max_mem: 17788M
[10/18 12:57:11] d2.utils.events INFO:  eta: 16:05:41  iter: 79  total_loss: 0.9342  loss_cls_stage0: 0.2155  loss_box_reg_stage0: 0.129  loss_cls_stage1: 0.1003  loss_box_reg_stage1: 0.04921  loss_cls_stage2: 0.07049  loss_box_reg_stage2: 0.01713  loss_rpn_cls: 0.1019  loss_rpn_loc: 0.05342    time: 0.6255  last_time: 0.6393  data_time: 0.0045  last_data_time: 0.0043   lr: 6.3237e-05  max_mem: 17788M
[10/18 12:57:23] d2.utils.events INFO:  eta: 16:06:55  iter: 99  total_loss: 1.099  loss_cls_stage0: 0.2496  loss_box_reg_stage0: 0.1514  loss_cls_stage1: 0.1337  loss_box_reg_stage1: 0.07808  loss_cls_stage2: 0.07755  loss_box_reg_stage2: 0.02982  loss_rpn_cls: 0.1765  loss_rpn_loc: 0.03968    time: 0.6260  last_time: 0.6296  data_time: 0.0049  last_data_time: 0.0044   lr: 7.9221e-05  max_mem: 17788M
[10/18 12:57:36] d2.utils.events INFO:  eta: 16:07:55  iter: 119  total_loss: 1.182  loss_cls_stage0: 0.3146  loss_box_reg_stage0: 0.1863  loss_cls_stage1: 0.164  loss_box_reg_stage1: 0.1169  loss_cls_stage2: 0.0968  loss_box_reg_stage2: 0.04625  loss_rpn_cls: 0.1533  loss_rpn_loc: 0.07122    time: 0.6267  last_time: 0.6219  data_time: 0.0060  last_data_time: 0.0042   lr: 9.5205e-05  max_mem: 17788M
[10/18 12:57:49] d2.utils.events INFO:  eta: 16:07:00  iter: 139  total_loss: 1.021  loss_cls_stage0: 0.3169  loss_box_reg_stage0: 0.1695  loss_cls_stage1: 0.1522  loss_box_reg_stage1: 0.07798  loss_cls_stage2: 0.07356  loss_box_reg_stage2: 0.03559  loss_rpn_cls: 0.1217  loss_rpn_loc: 0.06754    time: 0.6269  last_time: 0.6271  data_time: 0.0046  last_data_time: 0.0043   lr: 0.0001  max_mem: 17788M
[10/18 12:58:01] d2.utils.events INFO:  eta: 16:05:46  iter: 159  total_loss: 0.8649  loss_cls_stage0: 0.2284  loss_box_reg_stage0: 0.1487  loss_cls_stage1: 0.1154  loss_box_reg_stage1: 0.07957  loss_cls_stage2: 0.05644  loss_box_reg_stage2: 0.0168  loss_rpn_cls: 0.1014  loss_rpn_loc: 0.05698    time: 0.6269  last_time: 0.6381  data_time: 0.0043  last_data_time: 0.0041   lr: 0.0001  max_mem: 17788M
