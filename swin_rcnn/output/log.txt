[10/18 12:51:21] detectron2 INFO: Rank of current process: 0. World size: 1
[10/18 12:51:21] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]
numpy                            1.22.4
detectron2                       0.6 @/opt/conda/lib/python3.10/site-packages/detectron2
detectron2._C                    not built correctly: /opt/conda/lib/python3.10/site-packages/detectron2/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN2at4_ops7reshape4callERKNS_6TensorEN3c108ArrayRefINS5_6SymIntEEE
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
detectron2 arch flags            /opt/conda/lib/python3.10/site-packages/detectron2/_C.cpython-310-x86_64-linux-gnu.so
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.12.1+cu116 @/opt/conda/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            Tesla V100-SXM2-32GB (arch=7.0)
Driver version                   535.161.08
CUDA_HOME                        None - invalid!
Pillow                           10.4.0
torchvision                      0.13.1+cu116 @/opt/conda/lib/python3.10/site-packages/torchvision
torchvision arch flags           /opt/conda/lib/python3.10/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.8.1
-------------------------------  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/18 12:51:21] detectron2 INFO: Command line arguments: Namespace(config_file='ViTDet/configs/COCO/cascade_mask_rcnn_swin_l_in21k_50ep.py', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:49152', opts=[])
[10/18 12:51:21] detectron2 INFO: Contents of args.config_file=ViTDet/configs/COCO/cascade_mask_rcnn_swin_l_in21k_50ep.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcascade_mask_rcnn_swin_b_in21k_50ep[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15m([39m
[38;5;15m    [39m[38;5;15mdataloader[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mlr_multiplier[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mmodel[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mtrain[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15moptimizer[39m[38;5;15m,[39m
[38;5;15m)[39m


[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmapper[39m[38;5;197m.[39m[38;5;15muse_instance_mask[39m[38;5;197m=[39m[38;5;81mFalse[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmapper[39m[38;5;197m.[39m[38;5;15mrecompute_boxes[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mFalse[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtotal_batch_size[39m[38;5;197m=[39m[38;5;141m2[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdataset[39m[38;5;197m.[39m[38;5;15mnames[39m[38;5;197m=[39m[38;5;186m'[39m[38;5;186mcoco_trash_train[39m[38;5;186m'[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtest[39m[38;5;197m.[39m[38;5;15mdataset[39m[38;5;197m.[39m[38;5;15mnames[39m[38;5;197m=[39m[38;5;186m'[39m[38;5;186mcoco_trash_test[39m[38;5;186m'[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mevaluator[39m[38;5;197m.[39m[38;5;15mdataset_name[39m[38;5;197m=[39m[38;5;186m'[39m[38;5;186mcoco_trash_test[39m[38;5;186m'[39m


[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mdepths[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m[[39m[38;5;141m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m18[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m2[39m[38;5;15m][39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mdrop_path_rate[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15membed_dim[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m192[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mnum_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m[[39m[38;5;141m6[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m12[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m24[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m48[39m[38;5;15m][39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mroi_heads[39m[38;5;197m.[39m[38;5;15mnum_classes[39m[38;5;197m=[39m[38;5;141m10[39m

[38;5;81mdel[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mroi_heads[39m[38;5;15m[[39m[38;5;186m'[39m[38;5;186mmask_in_features[39m[38;5;186m'[39m[38;5;15m][39m
[38;5;81mdel[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mroi_heads[39m[38;5;15m[[39m[38;5;186m'[39m[38;5;186mmask_pooler[39m[38;5;186m'[39m[38;5;15m][39m
[38;5;81mdel[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mroi_heads[39m[38;5;15m[[39m[38;5;186m'[39m[38;5;186mmask_head[39m[38;5;186m'[39m[38;5;15m][39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15meval_period[39m[38;5;197m=[39m[38;5;141m100[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmax_iter[39m[38;5;197m=[39m[38;5;141m100[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15minit_checkpoint[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://ImageNetPretrained/swin/swin_large_patch4_window7_224_22k.pth[39m[38;5;186m"[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15moutput_dir[39m[38;5;197m=[39m[38;5;186m'[39m[38;5;186m/data/ephemeral/home/Seungcheol/level2-objectdetection-cv-07/swin_rcnn/output[39m[38;5;186m'[39m

[10/18 12:51:21] detectron2 INFO: Full config saved to /data/ephemeral/home/Seungcheol/level2-objectdetection-cv-07/swin_rcnn/output/config.yaml
[10/18 12:51:21] d2.utils.env INFO: Using a generated random seed 22087126
[10/18 12:51:25] detectron2 INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(
      192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
      (norm): LayerNorm()
    )
    (fpn_output2): Conv2d(
      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
      (norm): LayerNorm()
    )
    (fpn_lateral3): Conv2d(
      384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
      (norm): LayerNorm()
    )
    (fpn_output3): Conv2d(
      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
      (norm): LayerNorm()
    )
    (fpn_lateral4): Conv2d(
      768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
      (norm): LayerNorm()
    )
    (fpn_output4): Conv2d(
      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
      (norm): LayerNorm()
    )
    (fpn_lateral5): Conv2d(
      1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
      (norm): LayerNorm()
    )
    (fpn_output5): Conv2d(
      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
      (norm): LayerNorm()
    )
    (top_block): LastLevelMaxPool()
    (bottom_up): SwinTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=192, out_features=576, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=192, out_features=192, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): Identity()
              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=192, out_features=768, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=768, out_features=192, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock(
              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=192, out_features=576, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=192, out_features=192, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.017)
              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=192, out_features=768, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=768, out_features=192, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (downsample): PatchMerging(
            (reduction): Linear(in_features=768, out_features=384, bias=False)
            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.035)
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.052)
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (downsample): PatchMerging(
            (reduction): Linear(in_features=1536, out_features=768, bias=False)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.070)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.087)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (2): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.104)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (3): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.122)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (4): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.139)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (5): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.157)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (6): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.174)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (7): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.191)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (8): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.209)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (9): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.226)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (10): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.243)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (11): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.261)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (12): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.278)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (13): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.296)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (14): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.313)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (15): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.330)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (16): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.348)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (17): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.365)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (downsample): PatchMerging(
            (reduction): Linear(in_features=3072, out_features=1536, bias=False)
            (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=1536, out_features=4608, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=1536, out_features=1536, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.383)
              (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock(
              (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=1536, out_features=4608, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=1536, out_features=1536, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.400)
              (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
        )
      )
      (norm0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Sequential(
        (conv0): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): CascadeROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): ModuleList(
      (0): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (1): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (2): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
    )
    (box_predictor): ModuleList(
      (0): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=11, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (1): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=11, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (2): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=11, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
  )
)
[10/18 12:51:28] d2.data.datasets.coco INFO: Loaded 4437 images in COCO format from /data/ephemeral/home/Seungcheol/level2-objectdetection-cv-07/Split_data/train_0_10.json
[10/18 12:51:28] d2.data.build INFO: Removed 0 images with no usable annotations. 4437 images left.
[10/18 12:51:28] d2.data.build INFO: Distribution of instances among all 10 categories:
[36m|   category    | #instances   |  category   | #instances   |  category  | #instances   |
|:-------------:|:-------------|:-----------:|:-------------|:----------:|:-------------|
| General trash | 3523         |    Paper    | 5785         | Paper pack | 840          |
|     Metal     | 882          |    Glass    | 891          |  Plastic   | 2707         |
|   Styrofoam   | 1108         | Plastic bag | 4664         |  Battery   | 146          |
|   Clothing    | 412          |             |              |            |              |
|     total     | 20958        |             |              |            |              |[0m
[10/18 12:51:28] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [RandomFlip(), ResizeScale(min_scale=0.1, max_scale=2.0, target_height=1024, target_width=1024), FixedSizeCrop(crop_size=[1024, 1024], pad=False)]
[10/18 12:51:28] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/18 12:51:28] d2.data.common INFO: Serializing 4437 elements to byte tensors and concatenating them all ...
[10/18 12:51:28] d2.data.common INFO: Serialized dataset takes 2.07 MiB
[10/18 12:51:28] d2.data.build INFO: Making batched data loader with batch_size=2
[10/18 12:51:28] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from detectron2://ImageNetPretrained/swin/swin_large_patch4_window7_224_22k.pth ...
[10/18 12:51:28] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /data/ephemeral/home/.torch/iopath_cache/detectron2/ImageNetPretrained/swin/swin_large_patch4_window7_224_22k.pth ...
[10/18 12:51:29] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.bottom_up.layers.0.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.0.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.0.blocks.0.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.0.blocks.0.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.0.blocks.0.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.0.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.0.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.0.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.0.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.0.blocks.1.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.0.blocks.1.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.0.blocks.1.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.0.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.0.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.0.downsample.norm.{bias, weight}[0m
[34mbackbone.bottom_up.layers.0.downsample.reduction.weight[0m
[34mbackbone.bottom_up.layers.1.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.1.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.1.blocks.0.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.1.blocks.0.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.1.blocks.0.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.1.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.1.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.1.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.1.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.1.blocks.1.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.1.blocks.1.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.1.blocks.1.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.1.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.1.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.1.downsample.norm.{bias, weight}[0m
[34mbackbone.bottom_up.layers.1.downsample.reduction.weight[0m
[34mbackbone.bottom_up.layers.2.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.0.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.0.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.0.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.1.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.1.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.1.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.10.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.10.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.10.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.11.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.11.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.11.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.12.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.12.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.12.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.13.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.13.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.13.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.14.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.14.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.14.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.15.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.15.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.15.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.16.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.16.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.16.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.17.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.17.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.17.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.2.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.2.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.2.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.3.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.3.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.3.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.4.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.4.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.4.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.5.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.5.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.5.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.6.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.6.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.6.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.7.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.7.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.7.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.8.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.8.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.8.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.9.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.2.blocks.9.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.9.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.downsample.norm.{bias, weight}[0m
[34mbackbone.bottom_up.layers.2.downsample.reduction.weight[0m
[34mbackbone.bottom_up.layers.3.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.3.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.3.blocks.0.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.3.blocks.0.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.3.blocks.0.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.3.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.3.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.3.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.bottom_up.layers.3.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.bottom_up.layers.3.blocks.1.attn.{relative_position_bias_table, relative_position_index}[0m
[34mbackbone.bottom_up.layers.3.blocks.1.mlp.fc1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.3.blocks.1.mlp.fc2.{bias, weight}[0m
[34mbackbone.bottom_up.layers.3.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.layers.3.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.norm0.{bias, weight}[0m
[34mbackbone.bottom_up.norm1.{bias, weight}[0m
[34mbackbone.bottom_up.norm2.{bias, weight}[0m
[34mbackbone.bottom_up.norm3.{bias, weight}[0m
[34mbackbone.bottom_up.patch_embed.norm.{bias, weight}[0m
[34mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
[34mbackbone.fpn_lateral2.norm.{bias, weight}[0m
[34mbackbone.fpn_lateral2.weight[0m
[34mbackbone.fpn_lateral3.norm.{bias, weight}[0m
[34mbackbone.fpn_lateral3.weight[0m
[34mbackbone.fpn_lateral4.norm.{bias, weight}[0m
[34mbackbone.fpn_lateral4.weight[0m
[34mbackbone.fpn_lateral5.norm.{bias, weight}[0m
[34mbackbone.fpn_lateral5.weight[0m
[34mbackbone.fpn_output2.norm.{bias, weight}[0m
[34mbackbone.fpn_output2.weight[0m
[34mbackbone.fpn_output3.norm.{bias, weight}[0m
[34mbackbone.fpn_output3.weight[0m
[34mbackbone.fpn_output4.norm.{bias, weight}[0m
[34mbackbone.fpn_output4.weight[0m
[34mbackbone.fpn_output5.norm.{bias, weight}[0m
[34mbackbone.fpn_output5.weight[0m
[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.conv0.{bias, weight}[0m
[34mproposal_generator.rpn_head.conv.conv1.{bias, weight}[0m
[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}[0m
[34mroi_heads.box_head.0.conv1.norm.{bias, weight}[0m
[34mroi_heads.box_head.0.conv1.weight[0m
[34mroi_heads.box_head.0.conv2.norm.{bias, weight}[0m
[34mroi_heads.box_head.0.conv2.weight[0m
[34mroi_heads.box_head.0.conv3.norm.{bias, weight}[0m
[34mroi_heads.box_head.0.conv3.weight[0m
[34mroi_heads.box_head.0.conv4.norm.{bias, weight}[0m
[34mroi_heads.box_head.0.conv4.weight[0m
[34mroi_heads.box_head.0.fc1.{bias, weight}[0m
[34mroi_heads.box_head.1.conv1.norm.{bias, weight}[0m
[34mroi_heads.box_head.1.conv1.weight[0m
[34mroi_heads.box_head.1.conv2.norm.{bias, weight}[0m
[34mroi_heads.box_head.1.conv2.weight[0m
[34mroi_heads.box_head.1.conv3.norm.{bias, weight}[0m
[34mroi_heads.box_head.1.conv3.weight[0m
[34mroi_heads.box_head.1.conv4.norm.{bias, weight}[0m
[34mroi_heads.box_head.1.conv4.weight[0m
[34mroi_heads.box_head.1.fc1.{bias, weight}[0m
[34mroi_heads.box_head.2.conv1.norm.{bias, weight}[0m
[34mroi_heads.box_head.2.conv1.weight[0m
[34mroi_heads.box_head.2.conv2.norm.{bias, weight}[0m
[34mroi_heads.box_head.2.conv2.weight[0m
[34mroi_heads.box_head.2.conv3.norm.{bias, weight}[0m
[34mroi_heads.box_head.2.conv3.weight[0m
[34mroi_heads.box_head.2.conv4.norm.{bias, weight}[0m
[34mroi_heads.box_head.2.conv4.weight[0m
[34mroi_heads.box_head.2.fc1.{bias, weight}[0m
[34mroi_heads.box_predictor.0.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.0.cls_score.{bias, weight}[0m
[34mroi_heads.box_predictor.1.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.1.cls_score.{bias, weight}[0m
[34mroi_heads.box_predictor.2.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.2.cls_score.{bias, weight}[0m
[10/18 12:51:29] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mpatch_embed.proj.{bias, weight}[0m
  [35mpatch_embed.norm.{bias, weight}[0m
  [35mlayers.0.blocks.0.norm1.{bias, weight}[0m
  [35mlayers.0.blocks.0.attn.qkv.{bias, weight}[0m
  [35mlayers.0.blocks.0.attn.proj.{bias, weight}[0m
  [35mlayers.0.blocks.0.norm2.{bias, weight}[0m
  [35mlayers.0.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mlayers.0.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mlayers.0.blocks.1.norm1.{bias, weight}[0m
  [35mlayers.0.blocks.1.attn.qkv.{bias, weight}[0m
  [35mlayers.0.blocks.1.attn.proj.{bias, weight}[0m
  [35mlayers.0.blocks.1.norm2.{bias, weight}[0m
  [35mlayers.0.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mlayers.0.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mlayers.0.downsample.norm.{bias, weight}[0m
  [35mlayers.1.blocks.0.norm1.{bias, weight}[0m
  [35mlayers.1.blocks.0.attn.qkv.{bias, weight}[0m
  [35mlayers.1.blocks.0.attn.proj.{bias, weight}[0m
  [35mlayers.1.blocks.0.norm2.{bias, weight}[0m
  [35mlayers.1.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mlayers.1.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mlayers.1.blocks.1.norm1.{bias, weight}[0m
  [35mlayers.1.blocks.1.attn.qkv.{bias, weight}[0m
  [35mlayers.1.blocks.1.attn.proj.{bias, weight}[0m
  [35mlayers.1.blocks.1.norm2.{bias, weight}[0m
  [35mlayers.1.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mlayers.1.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mlayers.1.downsample.norm.{bias, weight}[0m
  [35mlayers.2.blocks.0.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.0.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.0.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.0.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.blocks.1.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.1.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.1.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.1.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.blocks.2.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.2.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.2.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.2.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.blocks.3.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.3.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.3.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.3.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.blocks.4.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.4.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.4.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.4.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.blocks.5.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.5.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.5.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.5.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.blocks.6.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.6.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.6.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.6.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.blocks.7.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.7.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.7.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.7.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.blocks.8.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.8.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.8.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.8.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.blocks.9.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.9.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.9.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.9.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.blocks.10.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.10.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.10.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.10.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.blocks.11.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.11.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.11.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.11.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.11.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.blocks.12.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.12.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.12.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.12.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.12.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.12.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.blocks.13.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.13.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.13.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.13.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.13.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.13.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.blocks.14.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.14.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.14.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.14.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.14.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.14.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.blocks.15.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.15.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.15.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.15.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.15.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.15.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.blocks.16.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.16.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.16.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.16.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.16.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.16.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.blocks.17.norm1.{bias, weight}[0m
  [35mlayers.2.blocks.17.attn.qkv.{bias, weight}[0m
  [35mlayers.2.blocks.17.attn.proj.{bias, weight}[0m
  [35mlayers.2.blocks.17.norm2.{bias, weight}[0m
  [35mlayers.2.blocks.17.mlp.fc1.{bias, weight}[0m
  [35mlayers.2.blocks.17.mlp.fc2.{bias, weight}[0m
  [35mlayers.2.downsample.norm.{bias, weight}[0m
  [35mlayers.3.blocks.0.norm1.{bias, weight}[0m
  [35mlayers.3.blocks.0.attn.qkv.{bias, weight}[0m
  [35mlayers.3.blocks.0.attn.proj.{bias, weight}[0m
  [35mlayers.3.blocks.0.norm2.{bias, weight}[0m
  [35mlayers.3.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mlayers.3.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mlayers.3.blocks.1.norm1.{bias, weight}[0m
  [35mlayers.3.blocks.1.attn.qkv.{bias, weight}[0m
  [35mlayers.3.blocks.1.attn.proj.{bias, weight}[0m
  [35mlayers.3.blocks.1.norm2.{bias, weight}[0m
  [35mlayers.3.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mlayers.3.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mnorm.{bias, weight}[0m
  [35mhead.{bias, weight}[0m
  [35mlayers.0.blocks.0.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.0.blocks.1.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.1.blocks.0.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.1.blocks.1.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.0.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.1.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.2.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.3.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.4.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.5.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.6.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.7.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.8.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.9.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.10.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.11.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.12.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.13.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.14.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.15.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.16.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.2.blocks.17.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.3.blocks.0.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.3.blocks.1.attn.{relative_position_bias_table, relative_position_index}[0m
  [35mlayers.0.blocks.1.attn_mask[0m
  [35mlayers.1.blocks.1.attn_mask[0m
  [35mlayers.2.blocks.1.attn_mask[0m
  [35mlayers.2.blocks.3.attn_mask[0m
  [35mlayers.2.blocks.5.attn_mask[0m
  [35mlayers.2.blocks.7.attn_mask[0m
  [35mlayers.2.blocks.9.attn_mask[0m
  [35mlayers.2.blocks.11.attn_mask[0m
  [35mlayers.2.blocks.13.attn_mask[0m
  [35mlayers.2.blocks.15.attn_mask[0m
  [35mlayers.2.blocks.17.attn_mask[0m
  [35mlayers.0.downsample.reduction.weight[0m
  [35mlayers.1.downsample.reduction.weight[0m
  [35mlayers.2.downsample.reduction.weight[0m
[10/18 12:51:29] d2.engine.train_loop INFO: Starting training from iteration 0
[10/18 12:51:43] d2.utils.events INFO:  eta: 0:00:50  iter: 19  total_loss: 1.747  loss_cls_stage0: 0.3022  loss_box_reg_stage0: 0.08246  loss_cls_stage1: 0.2098  loss_box_reg_stage1: 0.03378  loss_cls_stage2: 0.127  loss_box_reg_stage2: 0.0175  loss_rpn_cls: 0.6224  loss_rpn_loc: 0.0597    time: 0.6284  last_time: 0.6105  data_time: 0.0143  last_data_time: 0.0046   lr: 4e-05  max_mem: 17794M
[10/18 12:51:45] d2.engine.hooks INFO: Overall training speed: 21 iterations in 0:00:13 (0.6507 s / it)
[10/18 12:51:45] d2.engine.hooks INFO: Total training time: 0:00:13 (0:00:00 on hooks)
[10/18 12:51:45] d2.utils.events INFO:  eta: 0:00:47  iter: 23  total_loss: 1.365  loss_cls_stage0: 0.2816  loss_box_reg_stage0: 0.09117  loss_cls_stage1: 0.1536  loss_box_reg_stage1: 0.04404  loss_cls_stage2: 0.1137  loss_box_reg_stage2: 0.0175  loss_rpn_cls: 0.5752  loss_rpn_loc: 0.07195    time: 0.6284  last_time: 0.6220  data_time: 0.0042  last_data_time: 0.0039   lr: 4e-05  max_mem: 17794M
[10/18 12:53:58] detectron2 INFO: Rank of current process: 0. World size: 1
[10/18 12:53:58] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]
numpy                            1.22.4
detectron2                       0.6 @/opt/conda/lib/python3.10/site-packages/detectron2
detectron2._C                    not built correctly: /opt/conda/lib/python3.10/site-packages/detectron2/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN2at4_ops7reshape4callERKNS_6TensorEN3c108ArrayRefINS5_6SymIntEEE
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
detectron2 arch flags            /opt/conda/lib/python3.10/site-packages/detectron2/_C.cpython-310-x86_64-linux-gnu.so
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.12.1+cu116 @/opt/conda/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            Tesla V100-SXM2-32GB (arch=7.0)
Driver version                   535.161.08
CUDA_HOME                        None - invalid!
Pillow                           10.4.0
torchvision                      0.13.1+cu116 @/opt/conda/lib/python3.10/site-packages/torchvision
torchvision arch flags           /opt/conda/lib/python3.10/site-packages/torchvision/_C.so
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.8.1
-------------------------------  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/18 12:53:58] detectron2 INFO: Command line arguments: Namespace(config_file='ViTDet/configs/COCO/cascade_mask_rcnn_swin_l_in21k_50ep.py', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:49152', opts=[])
[10/18 12:53:58] detectron2 INFO: Contents of args.config_file=ViTDet/configs/COCO/cascade_mask_rcnn_swin_l_in21k_50ep.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcascade_mask_rcnn_swin_b_in21k_50ep[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15m([39m
[38;5;15m    [39m[38;5;15mdataloader[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mlr_multiplier[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mmodel[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mtrain[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15moptimizer[39m[38;5;15m,[39m
[38;5;15m)[39m


[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmapper[39m[38;5;197m.[39m[38;5;15muse_instance_mask[39m[38;5;197m=[39m[38;5;81mFalse[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmapper[39m[38;5;197m.[39m[38;5;15mrecompute_boxes[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mFalse[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtotal_batch_size[39m[38;5;197m=[39m[38;5;141m2[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdataset[39m[38;5;197m.[39m[38;5;15mnames[39m[38;5;197m=[39m[38;5;186m'[39m[38;5;186mcoco_trash_train[39m[38;5;186m'[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtest[39m[38;5;197m.[39m[38;5;15mdataset[39m[38;5;197m.[39m[38;5;15mnames[39m[38;5;197m=[39m[38;5;186m'[39m[38;5;186mcoco_trash_test[39m[38;5;186m'[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mevaluator[39m[38;5;197m.[39m[38;5;15mdataset_name[39m[38;5;197m=[39m[38;5;186m'[39m[38;5;186mcoco_trash_test[39m[38;5;186m'[39m


[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mdepths[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m[[39m[38;5;141m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m18[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m2[39m[38;5;15m][39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mdrop_path_rate[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15membed_dim[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m192[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mbackbone[39m[38;5;197m.[39m[38;5;15mbottom_up[39m[38;5;197m.[39m[38;5;15mnum_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m[[39m[38;5;141m6[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m12[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m24[39m[38;5;15m,[39m[38;5;15m [39m[38;5;141m48[39m[38;5;15m][39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mroi_heads[39m[38;5;197m.[39m[38;5;15mnum_classes[39m[38;5;197m=[39m[38;5;141m10[39m

[38;5;81mdel[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mroi_heads[39m[38;5;15m[[39m[38;5;186m'[39m[38;5;186mmask_in_features[39m[38;5;186m'[39m[38;5;15m][39m
[38;5;81mdel[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mroi_heads[39m[38;5;15m[[39m[38;5;186m'[39m[38;5;186mmask_pooler[39m[38;5;186m'[39m[38;5;15m][39m
[38;5;81mdel[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mroi_heads[39m[38;5;15m[[39m[38;5;186m'[39m[38;5;186mmask_head[39m[38;5;186m'[39m[38;5;15m][39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15meval_period[39m[38;5;197m=[39m[38;5;141m100[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmax_iter[39m[38;5;197m=[39m[38;5;141m100[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15minit_checkpoint[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mdetectron2://ImageNetPretrained/swin/swin_large_patch4_window7_224_22k.pth[39m[38;5;186m"[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15moutput_dir[39m[38;5;197m=[39m[38;5;186m'[39m[38;5;186m/data/ephemeral/home/Seungcheol/level2-objectdetection-cv-07/swin_rcnn/output[39m[38;5;186m'[39m

[38;5;15moptimizer[39m[38;5;197m.[39m[38;5;15mlr[39m[38;5;197m=[39m[38;5;141m0.0001[39m

[10/18 12:53:58] detectron2 INFO: Full config saved to /data/ephemeral/home/Seungcheol/level2-objectdetection-cv-07/swin_rcnn/output/config.yaml
[10/18 12:53:58] d2.utils.env INFO: Using a generated random seed 58830278
[10/18 12:54:02] detectron2 INFO: Model:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(
      192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
      (norm): LayerNorm()
    )
    (fpn_output2): Conv2d(
      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
      (norm): LayerNorm()
    )
    (fpn_lateral3): Conv2d(
      384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
      (norm): LayerNorm()
    )
    (fpn_output3): Conv2d(
      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
      (norm): LayerNorm()
    )
    (fpn_lateral4): Conv2d(
      768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
      (norm): LayerNorm()
    )
    (fpn_output4): Conv2d(
      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
      (norm): LayerNorm()
    )
    (fpn_lateral5): Conv2d(
      1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
      (norm): LayerNorm()
    )
    (fpn_output5): Conv2d(
      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
      (norm): LayerNorm()
    )
    (top_block): LastLevelMaxPool()
    (bottom_up): SwinTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=192, out_features=576, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=192, out_features=192, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): Identity()
              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=192, out_features=768, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=768, out_features=192, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock(
              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=192, out_features=576, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=192, out_features=192, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.017)
              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=192, out_features=768, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=768, out_features=192, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (downsample): PatchMerging(
            (reduction): Linear(in_features=768, out_features=384, bias=False)
            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.035)
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.052)
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (downsample): PatchMerging(
            (reduction): Linear(in_features=1536, out_features=768, bias=False)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.070)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.087)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (2): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.104)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (3): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.122)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (4): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.139)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (5): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.157)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (6): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.174)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (7): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.191)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (8): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.209)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (9): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.226)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (10): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.243)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (11): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.261)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (12): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.278)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (13): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.296)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (14): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.313)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (15): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.330)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (16): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.348)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (17): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.365)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (downsample): PatchMerging(
            (reduction): Linear(in_features=3072, out_features=1536, bias=False)
            (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=1536, out_features=4608, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=1536, out_features=1536, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.383)
              (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock(
              (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=1536, out_features=4608, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=1536, out_features=1536, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.400)
              (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                (act): GELU(approximate=none)
                (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
        )
      )
      (norm0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Sequential(
        (conv0): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (activation): ReLU()
        )
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): CascadeROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): ModuleList(
      (0): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (1): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
      (2): FastRCNNConvFCHead(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (conv3): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (conv4): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): LayerNorm()
          (activation): ReLU()
        )
        (flatten): Flatten(start_dim=1, end_dim=-1)
        (fc1): Linear(in_features=12544, out_features=1024, bias=True)
        (fc_relu1): ReLU()
      )
    )
    (box_predictor): ModuleList(
      (0): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=11, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (1): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=11, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
      (2): FastRCNNOutputLayers(
        (cls_score): Linear(in_features=1024, out_features=11, bias=True)
        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)
      )
    )
  )
)
