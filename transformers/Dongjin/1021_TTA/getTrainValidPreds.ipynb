{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForObjectDetection, Trainer\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from pycocotools.coco import COCO\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "import TTA\n",
    "from test_eval import test_eval\n",
    "\n",
    "sys.path.append(\"../1019_resume_train\")\n",
    "import utils\n",
    "import dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DetaForObjectDetection(\n",
       "  (model): DetaModel(\n",
       "    (backbone): DetaBackboneWithPositionalEncodings(\n",
       "      (model): SwinBackbone(\n",
       "        (embeddings): SwinEmbeddings(\n",
       "          (patch_embeddings): SwinPatchEmbeddings(\n",
       "            (projection): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))\n",
       "          )\n",
       "          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (encoder): SwinEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0-1): 2 x SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (key): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.1)\n",
       "                  (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=192, out_features=768, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=192, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (downsample): SwinPatchMerging(\n",
       "                (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "                (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0-1): 2 x SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.1)\n",
       "                  (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (downsample): SwinPatchMerging(\n",
       "                (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "                (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (2): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0-17): 18 x SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.1)\n",
       "                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (downsample): SwinPatchMerging(\n",
       "                (reduction): Linear(in_features=3072, out_features=1536, bias=False)\n",
       "                (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (3): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0-1): 2 x SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                      (key): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                      (value): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.1)\n",
       "                  (layernorm_after): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (hidden_states_norms): ModuleDict(\n",
       "          (stage2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (stage3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (stage4): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (position_embedding): DetaSinePositionEmbedding()\n",
       "    )\n",
       "    (input_proj): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Conv2d(1536, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (encoder): DetaEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x DetaEncoderLayer(\n",
       "          (self_attn): DetaMultiscaleDeformableAttention(\n",
       "            (sampling_offsets): Linear(in_features=256, out_features=320, bias=True)\n",
       "            (attention_weights): Linear(in_features=256, out_features=160, bias=True)\n",
       "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): DetaDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x DetaDecoderLayer(\n",
       "          (self_attn): DetaMultiheadAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): DetaMultiscaleDeformableAttention(\n",
       "            (sampling_offsets): Linear(in_features=256, out_features=320, bias=True)\n",
       "            (attention_weights): Linear(in_features=256, out_features=160, bias=True)\n",
       "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (bbox_embed): ModuleList(\n",
       "        (0-6): 7 x DetaMLPPredictionHead(\n",
       "          (layers): ModuleList(\n",
       "            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (class_embed): ModuleList(\n",
       "        (0-6): 7 x Linear(in_features=256, out_features=10, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (enc_output): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (pos_trans): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (pos_trans_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (pix_trans): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (pix_trans_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (class_embed): ModuleList(\n",
       "    (0-6): 7 x Linear(in_features=256, out_features=10, bias=True)\n",
       "  )\n",
       "  (bbox_embed): ModuleList(\n",
       "    (0-6): 7 x DetaMLPPredictionHead(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = '../1011_model_search/result/1015/jozhang97/deta-swin-large_3_img_size_720'\n",
    "coco_dir_path = '../../../Split_data'\n",
    "data_dir_path = '../../../../data/dataset'\n",
    "test_info_name = 'test.json'\n",
    "device = 'cuda'\n",
    "\n",
    "checkpoint_path = utils.find_checkpoint_path(model_path)\n",
    "run_name = os.path.split(model_path)[-1]\n",
    "json_path = os.path.join(model_path, run_name + '.json')\n",
    "\n",
    "# conf 파일 경로 덮어쓰기\n",
    "conf = utils.read_json(json_path)\n",
    "conf['coco_dir_path'] = coco_dir_path\n",
    "conf['data_dir_path'] = data_dir_path\n",
    "\n",
    "\n",
    "valid_info_path = os.path.join(conf['coco_dir_path'], conf['valid_info_name'])\n",
    "coco_valid = COCO(valid_info_path)\n",
    "valid = dataset.COCO2dataset(conf['data_dir_path'], coco_valid, range(10))\n",
    "\n",
    "test_info_path = os.path.join(conf['data_dir_path'], test_info_name)\n",
    "coco_test = COCO(test_info_path)\n",
    "test = dataset.COCO2dataset(conf['data_dir_path'], coco_test)\n",
    "\n",
    "\n",
    "# train_info_path = os.path.join(conf['coco_dir_path'], conf['train_info_name'])\n",
    "# coco_train = COCO(train_info_path)\n",
    "# train = dataset.COCO2dataset(conf['data_dir_path'], coco_train)\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint_path)\n",
    "model = AutoModelForObjectDetection.from_pretrained(checkpoint_path)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 10/11 [00:03<00:00,  2.74it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tta_transforms, tta_info \u001b[38;5;129;01min\u001b[39;00m tta_transforms_list:\n\u001b[1;32m     12\u001b[0m         save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dir_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtta_info\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 13\u001b[0m         \u001b[43mtest_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_processor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoco_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtta_transforms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m select \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tta_transforms, tta_info \u001b[38;5;129;01min\u001b[39;00m tta_transforms_list:\n",
      "File \u001b[0;32m~/project/Object detection/level2-objectdetection-cv-07/transformers/Dongjin/1021_TTA/test_eval/test_eval.py:13\u001b[0m, in \u001b[0;36mtest_eval\u001b[0;34m(model, image_processor, coco_data, data, tta, result_path, batch_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_index \u001b[38;5;129;01min\u001b[39;00m tqdm(batch_indices):\n\u001b[1;32m     12\u001b[0m     batch \u001b[38;5;241m=\u001b[39m data[batch_index]\n\u001b[0;32m---> 13\u001b[0m     image_name, prediction_string \u001b[38;5;241m=\u001b[39m \u001b[43mget_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_processor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoco_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     image_names\u001b[38;5;241m.\u001b[39mextend(image_name)\n\u001b[1;32m     16\u001b[0m     prediction_strings\u001b[38;5;241m.\u001b[39mextend(prediction_string)\n",
      "File \u001b[0;32m~/project/Object detection/level2-objectdetection-cv-07/transformers/Dongjin/1021_TTA/test_eval/test_eval.py:32\u001b[0m, in \u001b[0;36mget_predictions\u001b[0;34m(model, image_processor, coco_data, batch, tta, threshold, device)\u001b[0m\n\u001b[1;32m     29\u001b[0m image_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     30\u001b[0m image_infos \u001b[38;5;241m=\u001b[39m coco_data\u001b[38;5;241m.\u001b[39mloadImgs(image_ids)\n\u001b[0;32m---> 32\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     34\u001b[0m target_sizes \u001b[38;5;241m=\u001b[39m [image\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_project2/lib/python3.10/site-packages/transformers/image_processing_utils.py:41\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_project2/lib/python3.10/site-packages/transformers/models/deprecated/deta/image_processing_deta.py:1021\u001b[0m, in \u001b[0;36mDetaImageProcessor.preprocess\u001b[0;34m(self, images, annotations, return_segmentation_masks, masks_path, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_convert_annotations, do_pad, format, return_tensors, data_format, input_data_format, pad_size, **kwargs)\u001b[0m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;66;03m# Here, the pad() method pads to the maximum of (width, height). It does not need to be validated.\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m validate_preprocess_arguments(\n\u001b[1;32m   1011\u001b[0m     do_rescale\u001b[38;5;241m=\u001b[39mdo_rescale,\n\u001b[1;32m   1012\u001b[0m     rescale_factor\u001b[38;5;241m=\u001b[39mrescale_factor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     resample\u001b[38;5;241m=\u001b[39mresample,\n\u001b[1;32m   1019\u001b[0m )\n\u001b[0;32m-> 1021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_batched\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1022\u001b[0m     images \u001b[38;5;241m=\u001b[39m [images]\n\u001b[1;32m   1023\u001b[0m     annotations \u001b[38;5;241m=\u001b[39m [annotations] \u001b[38;5;28;01mif\u001b[39;00m annotations \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_project2/lib/python3.10/site-packages/transformers/image_utils.py:157\u001b[0m, in \u001b[0;36mis_batched\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_batched\u001b[39m(img):\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 157\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m is_valid_image(\u001b[43mimg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "select = 'valid'\n",
    "save_dir_path = f'result/TTA/{select}'\n",
    "os.makedirs(save_dir_path, exist_ok=True)\n",
    "\n",
    "tta_transforms_list = [\n",
    "        (TTA.Compose([TTA.horizontal_flip]), 'hflip'),\n",
    "        (TTA.Compose([TTA.identity]), 'identity')\n",
    "    ]\n",
    "\n",
    "if select == 'valid':\n",
    "    for tta_transforms, tta_info in tta_transforms_list:\n",
    "        save_path = f'{save_dir_path}/{run_name}_{tta_info}.csv'\n",
    "        test_eval(model, image_processor, coco_valid, valid, tta_transforms, save_path)\n",
    "\n",
    "if select == 'test':\n",
    "    for tta_transforms, tta_info in tta_transforms_list:\n",
    "        save_path = f'{save_dir_path}/{run_name}_{tta_info}.csv'\n",
    "        test_eval(model, image_processor, coco_valid, valid, tta_transforms, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_project2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
