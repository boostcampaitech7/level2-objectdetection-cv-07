{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d28a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9614999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import detectron2\n",
    "from detectron2.data import detection_utils as utils\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89687c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 데이터셋 삭제\n",
    "dataset_name = 'coco_trash_final'\n",
    "if dataset_name in DatasetCatalog:\n",
    "    DatasetCatalog.remove(dataset_name)\n",
    "if dataset_name in MetadataCatalog:\n",
    "    MetadataCatalog.remove(dataset_name)\n",
    "\n",
    "# Register Dataset\n",
    "try:\n",
    "    register_coco_instances('coco_trash_final', {}, '/data/ephemeral/home/data/dataset/test.json', '/data/ephemeral/home/data/dataset/')\n",
    "except AssertionError:\n",
    "    pass\n",
    "\n",
    "# try:\n",
    "#     register_coco_instances('coco_trash_final', {}, '/data/ephemeral/home/Seungcheol/level2-objectdetection-cv-07/Split_data/valid_0_10.json', '/data/ephemeral/home/data/dataset/')\n",
    "# except AssertionError:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "594f0617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/21 17:01:15 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from /data/ephemeral/home/Seungcheol/level2-objectdetection-cv-07/vitdet_rcnn_cascade/output/model_0049999.pth ...\n",
      "\u001b[32m[10/21 17:01:17 d2.data.datasets.coco]: \u001b[0mLoaded 4871 images in COCO format from /data/ephemeral/home/data/dataset/test.json\n",
      "\u001b[32m[10/21 17:01:17 d2.data.build]: \u001b[0mDistribution of instances among all 10 categories:\n",
      "\u001b[36m|   category    | #instances   |  category   | #instances   |  category  | #instances   |\n",
      "|:-------------:|:-------------|:-----------:|:-------------|:----------:|:-------------|\n",
      "| General trash | 0            |    Paper    | 0            | Paper pack | 0            |\n",
      "|     Metal     | 0            |    Glass    | 0            |  Plastic   | 0            |\n",
      "|   Styrofoam   | 0            | Plastic bag | 0            |  Battery   | 0            |\n",
      "|   Clothing    | 0            |             |              |            |              |\n",
      "|     total     | 0            |             |              |            |              |\u001b[0m\n",
      "\u001b[32m[10/21 17:01:17 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1024, 1024), max_size=1024)]\n",
      "\u001b[32m[10/21 17:01:17 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[10/21 17:01:17 d2.data.common]: \u001b[0mSerializing 4871 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[10/21 17:01:17 d2.data.common]: \u001b[0mSerialized dataset takes 0.61 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4871 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 768.00 MiB (GPU 0; 31.74 GiB total capacity; 2.11 GiB already allocated; 132.38 MiB free; 2.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 48\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstances\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# model에 올바른 형식으로 전달\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# 예측 결과 처리\u001b[39;00m\n\u001b[1;32m     51\u001b[0m targets \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mpred_classes\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py:150\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, batched_inputs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    batched_inputs: a list, batched outputs of :class:`DatasetMapper` .\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m        \"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\", \"pred_keypoints\"\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_image(batched_inputs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstances\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batched_inputs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py:204\u001b[0m, in \u001b[0;36mGeneralizedRCNN.inference\u001b[0;34m(self, batched_inputs, detected_instances, do_postprocess)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining\n\u001b[1;32m    203\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_image(batched_inputs)\n\u001b[0;32m--> 204\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m detected_instances \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproposal_generator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/modeling/backbone/vit.py:489\u001b[0m, in \u001b[0;36mSimpleFeaturePyramid.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    478\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;124;03m        x: Tensor of shape (N,C,H,W). H, W must be a multiple of ``self.size_divisibility``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;124;03m            [\"p2\", \"p3\", ..., \"p6\"].\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m     bottom_up_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m     features \u001b[38;5;241m=\u001b[39m bottom_up_features[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_feature]\n\u001b[1;32m    491\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/modeling/backbone/vit.py:357\u001b[0m, in \u001b[0;36mViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    352\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m get_abs_pos(\n\u001b[1;32m    353\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrain_use_cls_token, (x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m    354\u001b[0m     )\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 357\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m outputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_out_features[\u001b[38;5;241m0\u001b[39m]: x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)}\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/modeling/backbone/vit.py:218\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    215\u001b[0m     H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    216\u001b[0m     x, pad_hw \u001b[38;5;241m=\u001b[39m window_partition(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size)\n\u001b[0;32m--> 218\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# Reverse window partition\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/modeling/backbone/vit.py:75\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     72\u001b[0m attn \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale) \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_rel_pos:\n\u001b[0;32m---> 75\u001b[0m     attn \u001b[38;5;241m=\u001b[39m \u001b[43madd_decomposed_rel_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     78\u001b[0m x \u001b[38;5;241m=\u001b[39m (attn \u001b[38;5;241m@\u001b[39m v)\u001b[38;5;241m.\u001b[39mview(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/modeling/backbone/utils.py:122\u001b[0m, in \u001b[0;36madd_decomposed_rel_pos\u001b[0;34m(attn, q, rel_pos_h, rel_pos_w, q_size, k_size)\u001b[0m\n\u001b[1;32m    118\u001b[0m rel_h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbhwc,hkc->bhwk\u001b[39m\u001b[38;5;124m\"\u001b[39m, r_q, Rh)\n\u001b[1;32m    119\u001b[0m rel_w \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbhwc,wkc->bhwk\u001b[39m\u001b[38;5;124m\"\u001b[39m, r_q, Rw)\n\u001b[1;32m    121\u001b[0m attn \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 122\u001b[0m     \u001b[43mattn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_w\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrel_h\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrel_w\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    123\u001b[0m )\u001b[38;5;241m.\u001b[39mview(B, q_h \u001b[38;5;241m*\u001b[39m q_w, k_h \u001b[38;5;241m*\u001b[39m k_w)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 768.00 MiB (GPU 0; 31.74 GiB total capacity; 2.11 GiB already allocated; 132.38 MiB free; 2.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from detectron2.engine import (\n",
    "    AMPTrainer,\n",
    "    SimpleTrainer,\n",
    "    default_argument_parser,\n",
    "    default_setup,\n",
    "    default_writers,\n",
    "    hooks,\n",
    "    launch,\n",
    ")\n",
    "from detectron2.modeling import GeneralizedRCNNWithTTA, GeneralizedRCNN\n",
    "import logging\n",
    "from detectron2.engine.defaults import create_ddp_model\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.evaluation import inference_on_dataset, print_csv_format\n",
    "from detectron2.config import LazyConfig, instantiate\n",
    "from omegaconf import OmegaConf\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# 설정 파일 로드 (LazyConfig)\n",
    "cfg = LazyConfig.load('/data/ephemeral/home/Seungcheol/level2-objectdetection-cv-07/vitdet_rcnn_cascade/output/config.yaml')\n",
    "cfg.dataloader.test.dataset.names = 'coco_trash_final'\n",
    "\n",
    "model = instantiate(cfg.model)\n",
    "model.to(cfg.train.device)\n",
    "model = create_ddp_model(model)\n",
    "# 모델 가중치 로드\n",
    "DetectionCheckpointer(model).load('/data/ephemeral/home/Seungcheol/level2-objectdetection-cv-07/vitdet_rcnn_cascade/output/model_0049999.pth')\n",
    "\n",
    "# 테스트 데이터셋 로드\n",
    "test_loader = instantiate(cfg.dataloader.test)\n",
    "\n",
    "# 모델을 평가 모드로 설정\n",
    "model.eval()\n",
    "\n",
    "# 예측 수행\n",
    "prediction_strings = []\n",
    "file_names = []\n",
    "\n",
    "for data in tqdm(test_loader):\n",
    "    \n",
    "    prediction_string = ''\n",
    "    input=data[0]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(data)[0]['instances']  # model에 올바른 형식으로 전달\n",
    "    \n",
    "    # 예측 결과 처리\n",
    "    targets = outputs.pred_classes.cpu().tolist()\n",
    "    boxes = [i.cpu().detach().numpy() for i in outputs.pred_boxes]\n",
    "    scores = outputs.scores.cpu().tolist()\n",
    "    \n",
    "    for target, box, score in zip(targets, boxes, scores):\n",
    "        prediction_string += (str(target) + ' ' + str(score) + ' ' + str(box[0]) + ' ' \n",
    "        + str(box[1]) + ' ' + str(box[2]) + ' ' + str(box[3]) + ' ')\n",
    "    \n",
    "    prediction_strings.append(prediction_string)\n",
    "    file_names.append(input['file_name'].replace('/data/ephemeral/home/data/dataset/', ''))\n",
    "\n",
    "# 제출 파일 생성\n",
    "submission = pd.DataFrame()\n",
    "submission['PredictionString'] = prediction_strings\n",
    "submission['image_id'] = file_names\n",
    "submission.to_csv(os.path.join(cfg.train.output_dir, 'submission_det2.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a260a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/21 17:16:12 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from /data/ephemeral/home/Seungcheol/level2-objectdetection-cv-07/vitdet_rcnn_cascade/output/model_0049999.pth ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mmodel.backbone.net.blocks.0.attn.proj.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.0.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.0.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.0.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.0.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.0.norm1.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.0.norm2.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.1.attn.proj.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.1.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.1.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.1.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.1.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.1.norm1.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.1.norm2.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.10.attn.proj.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.10.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.10.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.10.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.10.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.10.norm1.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.10.norm2.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.11.attn.proj.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.11.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.11.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.11.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.11.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.11.norm1.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.11.norm2.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.2.attn.proj.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.2.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.2.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.2.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.2.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.2.norm1.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.2.norm2.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.3.attn.proj.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.3.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.3.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.3.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.3.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.3.norm1.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.3.norm2.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.4.attn.proj.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.4.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.4.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.4.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.4.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.4.norm1.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.4.norm2.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.5.attn.proj.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.5.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.5.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.5.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.5.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.5.norm1.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.5.norm2.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.6.attn.proj.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.6.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.6.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.6.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.6.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.6.norm1.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.6.norm2.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.7.attn.proj.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.7.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.7.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.7.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.7.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.7.norm1.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.7.norm2.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.8.attn.proj.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.8.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.8.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.8.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.8.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.8.norm1.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.8.norm2.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.9.attn.proj.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.9.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.9.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.9.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.9.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.9.norm1.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.blocks.9.norm2.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.patch_embed.proj.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.net.pos_embed\u001b[0m\n",
      "\u001b[34mmodel.backbone.simfp_2.0.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.simfp_2.1.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.simfp_2.3.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.simfp_2.4.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.simfp_2.4.weight\u001b[0m\n",
      "\u001b[34mmodel.backbone.simfp_2.5.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.simfp_2.5.weight\u001b[0m\n",
      "\u001b[34mmodel.backbone.simfp_3.0.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.simfp_3.1.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.simfp_3.1.weight\u001b[0m\n",
      "\u001b[34mmodel.backbone.simfp_3.2.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.simfp_3.2.weight\u001b[0m\n",
      "\u001b[34mmodel.backbone.simfp_4.0.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.simfp_4.0.weight\u001b[0m\n",
      "\u001b[34mmodel.backbone.simfp_4.1.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.simfp_4.1.weight\u001b[0m\n",
      "\u001b[34mmodel.backbone.simfp_5.1.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.simfp_5.1.weight\u001b[0m\n",
      "\u001b[34mmodel.backbone.simfp_5.2.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.backbone.simfp_5.2.weight\u001b[0m\n",
      "\u001b[34mmodel.proposal_generator.rpn_head.anchor_deltas.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.proposal_generator.rpn_head.conv.conv0.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.proposal_generator.rpn_head.conv.conv1.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.proposal_generator.rpn_head.objectness_logits.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.roi_heads.box_head.0.conv1.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.roi_heads.box_head.0.conv1.weight\u001b[0m\n",
      "\u001b[34mmodel.roi_heads.box_head.0.conv2.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.roi_heads.box_head.0.conv2.weight\u001b[0m\n",
      "\u001b[34mmodel.roi_heads.box_head.0.conv3.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.roi_heads.box_head.0.conv3.weight\u001b[0m\n",
      "\u001b[34mmodel.roi_heads.box_head.0.conv4.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.roi_heads.box_head.0.conv4.weight\u001b[0m\n",
      "\u001b[34mmodel.roi_heads.box_head.0.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.roi_heads.box_head.1.conv1.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.roi_heads.box_head.1.conv1.weight\u001b[0m\n",
      "\u001b[34mmodel.roi_heads.box_head.1.conv2.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.roi_heads.box_head.1.conv2.weight\u001b[0m\n",
      "\u001b[34mmodel.roi_heads.box_head.1.conv3.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.roi_heads.box_head.1.conv3.weight\u001b[0m\n",
      "\u001b[34mmodel.roi_heads.box_head.1.conv4.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.roi_heads.box_head.1.conv4.weight\u001b[0m\n",
      "\u001b[34mmodel.roi_heads.box_head.1.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.roi_heads.box_head.2.conv1.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.roi_heads.box_head.2.conv1.weight\u001b[0m\n",
      "\u001b[34mmodel.roi_heads.box_head.2.conv2.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.roi_heads.box_head.2.conv2.weight\u001b[0m\n",
      "\u001b[34mmodel.roi_heads.box_head.2.conv3.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.roi_heads.box_head.2.conv3.weight\u001b[0m\n",
      "\u001b[34mmodel.roi_heads.box_head.2.conv4.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.roi_heads.box_head.2.conv4.weight\u001b[0m\n",
      "\u001b[34mmodel.roi_heads.box_head.2.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.roi_heads.box_predictor.0.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.roi_heads.box_predictor.0.cls_score.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.roi_heads.box_predictor.1.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.roi_heads.box_predictor.1.cls_score.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.roi_heads.box_predictor.2.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodel.roi_heads.box_predictor.2.cls_score.{bias, weight}\u001b[0m\n",
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mbackbone.simfp_2.0.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.simfp_2.1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.simfp_2.3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.simfp_2.4.weight\u001b[0m\n",
      "  \u001b[35mbackbone.simfp_2.4.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.simfp_2.5.weight\u001b[0m\n",
      "  \u001b[35mbackbone.simfp_2.5.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.simfp_3.0.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.simfp_3.1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.simfp_3.1.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.simfp_3.2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.simfp_3.2.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.simfp_4.0.weight\u001b[0m\n",
      "  \u001b[35mbackbone.simfp_4.0.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.simfp_4.1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.simfp_4.1.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.simfp_5.1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.simfp_5.1.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.simfp_5.2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.simfp_5.2.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.pos_embed\u001b[0m\n",
      "  \u001b[35mbackbone.net.patch_embed.proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.0.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.0.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.0.attn.qkv.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.0.attn.proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.0.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.0.mlp.fc1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.0.mlp.fc2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.1.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.1.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.1.attn.qkv.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.1.attn.proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.1.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.1.mlp.fc1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.1.mlp.fc2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.2.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.2.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.2.attn.qkv.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.2.attn.proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.2.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.2.mlp.fc1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.2.mlp.fc2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.3.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.3.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.3.attn.qkv.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.3.attn.proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.3.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.3.mlp.fc1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.3.mlp.fc2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.4.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.4.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.4.attn.qkv.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.4.attn.proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.4.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.4.mlp.fc1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.4.mlp.fc2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.5.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.5.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.5.attn.qkv.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.5.attn.proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.5.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.5.mlp.fc1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.5.mlp.fc2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.6.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.6.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.6.attn.qkv.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.6.attn.proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.6.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.6.mlp.fc1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.6.mlp.fc2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.7.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.7.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.7.attn.qkv.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.7.attn.proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.7.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.7.mlp.fc1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.7.mlp.fc2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.8.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.8.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.8.attn.qkv.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.8.attn.proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.8.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.8.mlp.fc1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.8.mlp.fc2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.9.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.9.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.9.attn.qkv.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.9.attn.proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.9.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.9.mlp.fc1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.9.mlp.fc2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.10.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.10.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.10.attn.qkv.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.10.attn.proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.10.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.10.mlp.fc1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.10.mlp.fc2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.11.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.11.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.11.attn.qkv.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.11.attn.proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.11.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.11.mlp.fc1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.11.mlp.fc2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mproposal_generator.rpn_head.conv.conv0.{bias, weight}\u001b[0m\n",
      "  \u001b[35mproposal_generator.rpn_head.conv.conv1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mproposal_generator.rpn_head.objectness_logits.{bias, weight}\u001b[0m\n",
      "  \u001b[35mproposal_generator.rpn_head.anchor_deltas.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.0.conv1.weight\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.0.conv1.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.0.conv2.weight\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.0.conv2.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.0.conv3.weight\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.0.conv3.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.0.conv4.weight\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.0.conv4.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.0.fc1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.1.conv1.weight\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.1.conv1.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.1.conv2.weight\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.1.conv2.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.1.conv3.weight\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.1.conv3.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.1.conv4.weight\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.1.conv4.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.1.fc1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.2.conv1.weight\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.2.conv1.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.2.conv2.weight\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.2.conv2.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.2.conv3.weight\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.2.conv3.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.2.conv4.weight\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.2.conv4.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.2.fc1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.box_predictor.0.cls_score.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.box_predictor.0.bbox_pred.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.box_predictor.1.cls_score.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.box_predictor.1.bbox_pred.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.box_predictor.2.cls_score.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.box_predictor.2.bbox_pred.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/21 17:16:14 d2.data.datasets.coco]: \u001b[0mLoaded 4871 images in COCO format from /data/ephemeral/home/data/dataset/test.json\n",
      "\u001b[32m[10/21 17:16:14 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1024, 1024), max_size=1024)]\n",
      "\u001b[32m[10/21 17:16:14 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[10/21 17:16:14 d2.data.common]: \u001b[0mSerializing 4871 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[10/21 17:16:14 d2.data.common]: \u001b[0mSerialized dataset takes 0.61 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4871 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 132.00 MiB (GPU 0; 31.74 GiB total capacity; 2.00 GiB already allocated; 30.38 MiB free; 2.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 58\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstances\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# model에 올바른 형식으로 전달\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# 예측 결과 처리\u001b[39;00m\n\u001b[1;32m     61\u001b[0m targets \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mpred_classes\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/modeling/test_time_augmentation.py:204\u001b[0m, in \u001b[0;36mGeneralizedRCNNWithTTA.__call__\u001b[0;34m(self, batched_inputs)\u001b[0m\n\u001b[1;32m    201\u001b[0m         ret[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_one_image(_maybe_read_image(x)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m batched_inputs]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/modeling/test_time_augmentation.py:204\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    201\u001b[0m         ret[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_one_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_maybe_read_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m batched_inputs]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/modeling/test_time_augmentation.py:219\u001b[0m, in \u001b[0;36mGeneralizedRCNNWithTTA._inference_one_image\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# Detect boxes from all augmented versions\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_turn_off_roi_heads([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask_on\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeypoint_on\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# temporarily disable roi heads\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m     all_boxes, all_scores, all_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_augmented_boxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43maugmented_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtfms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# merge all detected boxes to obtain final predictions for boxes\u001b[39;00m\n\u001b[1;32m    221\u001b[0m merged_instances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_detections(all_boxes, all_scores, all_classes, orig_shape)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/modeling/test_time_augmentation.py:246\u001b[0m, in \u001b[0;36mGeneralizedRCNNWithTTA._get_augmented_boxes\u001b[0;34m(self, augmented_inputs, tfms)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_augmented_boxes\u001b[39m(\u001b[38;5;28mself\u001b[39m, augmented_inputs, tfms):\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# 1: forward with all augmented images\u001b[39;00m\n\u001b[0;32m--> 246\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43maugmented_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;66;03m# 2: union the results\u001b[39;00m\n\u001b[1;32m    248\u001b[0m     all_boxes \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/modeling/test_time_augmentation.py:179\u001b[0m, in \u001b[0;36mGeneralizedRCNNWithTTA._batch_inference\u001b[0;34m(self, batched_inputs, detected_instances)\u001b[0m\n\u001b[1;32m    176\u001b[0m     instances\u001b[38;5;241m.\u001b[39mappend(instance)\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;129;01mor\u001b[39;00m idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(batched_inputs) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    178\u001b[0m         outputs\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m--> 179\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m                \u001b[49m\u001b[43minstances\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minstances\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdo_postprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m         )\n\u001b[1;32m    185\u001b[0m         inputs, instances \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py:204\u001b[0m, in \u001b[0;36mGeneralizedRCNN.inference\u001b[0;34m(self, batched_inputs, detected_instances, do_postprocess)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining\n\u001b[1;32m    203\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_image(batched_inputs)\n\u001b[0;32m--> 204\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m detected_instances \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproposal_generator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/modeling/backbone/vit.py:489\u001b[0m, in \u001b[0;36mSimpleFeaturePyramid.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    478\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;124;03m        x: Tensor of shape (N,C,H,W). H, W must be a multiple of ``self.size_divisibility``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;124;03m            [\"p2\", \"p3\", ..., \"p6\"].\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m     bottom_up_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m     features \u001b[38;5;241m=\u001b[39m bottom_up_features[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_feature]\n\u001b[1;32m    491\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/modeling/backbone/vit.py:357\u001b[0m, in \u001b[0;36mViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    352\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m get_abs_pos(\n\u001b[1;32m    353\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrain_use_cls_token, (x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m    354\u001b[0m     )\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 357\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m outputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_out_features[\u001b[38;5;241m0\u001b[39m]: x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)}\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/modeling/backbone/vit.py:218\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    215\u001b[0m     H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    216\u001b[0m     x, pad_hw \u001b[38;5;241m=\u001b[39m window_partition(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size)\n\u001b[0;32m--> 218\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# Reverse window partition\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/modeling/backbone/vit.py:75\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     72\u001b[0m attn \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale) \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_rel_pos:\n\u001b[0;32m---> 75\u001b[0m     attn \u001b[38;5;241m=\u001b[39m \u001b[43madd_decomposed_rel_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     78\u001b[0m x \u001b[38;5;241m=\u001b[39m (attn \u001b[38;5;241m@\u001b[39m v)\u001b[38;5;241m.\u001b[39mview(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/modeling/backbone/utils.py:122\u001b[0m, in \u001b[0;36madd_decomposed_rel_pos\u001b[0;34m(attn, q, rel_pos_h, rel_pos_w, q_size, k_size)\u001b[0m\n\u001b[1;32m    118\u001b[0m rel_h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbhwc,hkc->bhwk\u001b[39m\u001b[38;5;124m\"\u001b[39m, r_q, Rh)\n\u001b[1;32m    119\u001b[0m rel_w \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbhwc,wkc->bhwk\u001b[39m\u001b[38;5;124m\"\u001b[39m, r_q, Rw)\n\u001b[1;32m    121\u001b[0m attn \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 122\u001b[0m     \u001b[43mattn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_w\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrel_h\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrel_w\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    123\u001b[0m )\u001b[38;5;241m.\u001b[39mview(B, q_h \u001b[38;5;241m*\u001b[39m q_w, k_h \u001b[38;5;241m*\u001b[39m k_w)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 132.00 MiB (GPU 0; 31.74 GiB total capacity; 2.00 GiB already allocated; 30.38 MiB free; 2.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from detectron2.engine import (\n",
    "    AMPTrainer,\n",
    "    SimpleTrainer,\n",
    "    default_argument_parser,\n",
    "    default_setup,\n",
    "    default_writers,\n",
    "    hooks,\n",
    "    launch,\n",
    ")\n",
    "from detectron2.modeling import GeneralizedRCNNWithTTA, GeneralizedRCNN\n",
    "import logging\n",
    "from detectron2.engine.defaults import create_ddp_model\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.evaluation import inference_on_dataset, print_csv_format\n",
    "from detectron2.config import LazyConfig, instantiate\n",
    "from omegaconf import OmegaConf\n",
    "from detectron2.config import get_cfg\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# 설정 파일 로드 (LazyConfig)\n",
    "cfg = LazyConfig.load('/data/ephemeral/home/Seungcheol/level2-objectdetection-cv-07/vitdet_rcnn_cascade/output/config.yaml')\n",
    "cfg.dataloader.test.dataset.names = 'coco_trash_final'\n",
    "\n",
    "model = instantiate(cfg.model)\n",
    "model.to(cfg.train.device)\n",
    "\n",
    "# tta_cfg를 CfgNode로 생성\n",
    "tta_cfg = get_cfg()\n",
    "tta_cfg.TEST.AUG.ENABLED = True\n",
    "tta_cfg.TEST.AUG.MIN_SIZES = (400, 500, 600, 700, 800)\n",
    "tta_cfg.TEST.AUG.MAX_SIZE = 1333\n",
    "tta_cfg.TEST.AUG.FLIP = True\n",
    "tta_cfg.MODEL.KEYPOINT_ON = False\n",
    "tta_cfg.MODEL.LOAD_PROPOSALS = False\n",
    "tta_cfg.MODEL.ROI_HEADS.NUM_CLASSES = 10  # 클래스 수에 맞게 조정\n",
    "tta_cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.5\n",
    "tta_cfg.TEST.DETECTIONS_PER_IMAGE = 100\n",
    "\n",
    "# 기존 cfg에서 필요한 설정을 복사\n",
    "tta_cfg.MODEL.WEIGHTS = cfg.MODEL.WEIGHTS\n",
    "tta_cfg.MODEL.DEVICE = cfg.MODEL.DEVICE\n",
    "\n",
    "model = GeneralizedRCNNWithTTA(tta_cfg, model)\n",
    "\n",
    "model = GeneralizedRCNNWithTTA(tta_cfg, model)\n",
    "model = create_ddp_model(model)\n",
    "# 모델 가중치 로드\n",
    "DetectionCheckpointer(model).load('/data/ephemeral/home/Seungcheol/level2-objectdetection-cv-07/vitdet_rcnn_cascade/output/model_0049999.pth')\n",
    "\n",
    "# 테스트 데이터셋 로드\n",
    "test_loader = instantiate(cfg.dataloader.test)\n",
    "\n",
    "# 모델을 평가 모드로 설정\n",
    "model.eval()\n",
    "\n",
    "# 예측 수행\n",
    "prediction_strings = []\n",
    "file_names = []\n",
    "\n",
    "for data in tqdm(test_loader):\n",
    "    \n",
    "    prediction_string = ''\n",
    "    input=data[0]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(data)[0]['instances']  # model에 올바른 형식으로 전달\n",
    "    \n",
    "    # 예측 결과 처리\n",
    "    targets = outputs.pred_classes.cpu().tolist()\n",
    "    boxes = [i.cpu().detach().numpy() for i in outputs.pred_boxes]\n",
    "    scores = outputs.scores.cpu().tolist()\n",
    "    \n",
    "    for target, box, score in zip(targets, boxes, scores):\n",
    "        prediction_string += (str(target) + ' ' + str(score) + ' ' + str(box[0]) + ' ' \n",
    "        + str(box[1]) + ' ' + str(box[2]) + ' ' + str(box[3]) + ' ')\n",
    "    \n",
    "    prediction_strings.append(prediction_string)\n",
    "    file_names.append(input['file_name'].replace('/data/ephemeral/home/data/dataset/', ''))\n",
    "\n",
    "# 제출 파일 생성\n",
    "submission = pd.DataFrame()\n",
    "submission['PredictionString'] = prediction_strings\n",
    "submission['image_id'] = file_names\n",
    "submission.to_csv(os.path.join(cfg.train.output_dir, 'submission_det4.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766d1ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
